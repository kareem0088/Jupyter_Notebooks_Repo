{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "275408b0-1dcf-436b-859f-01b9949a206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "io = 'lklklk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e49a1da8-1c10-42b2-8916-d0604e5e189f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io.find(\"llk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb48f0af-8d7e-4f29-b2fd-2bacb944fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tickterial\n",
      "  Using cached tickterial-1.1.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting loguru==0.7.2 (from tickterial)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pytz==2022.7 (from tickterial)\n",
      "  Downloading pytz-2022.7-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting requests==2.32.2 (from tickterial)\n",
      "  Downloading requests-2.32.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting alive-progress>=3.1.5 (from tickterial)\n",
      "  Downloading alive_progress-3.2.0-py3-none-any.whl.metadata (70 kB)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\access\\appdata\\roaming\\python\\python312\\site-packages (from loguru==0.7.2->tickterial) (0.4.6)\n",
      "Collecting win32-setctime>=1.0.0 (from loguru==0.7.2->tickterial)\n",
      "  Downloading win32_setctime-1.2.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests==2.32.2->tickterial) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests==2.32.2->tickterial) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\access\\appdata\\roaming\\python\\python312\\site-packages (from requests==2.32.2->tickterial) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests==2.32.2->tickterial) (2025.4.26)\n",
      "Collecting about-time==4.2.1 (from alive-progress>=3.1.5->tickterial)\n",
      "  Downloading about_time-4.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting grapheme==0.6.0 (from alive-progress>=3.1.5->tickterial)\n",
      "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading tickterial-1.1.2-py3-none-any.whl (9.8 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "Downloading requests-2.32.2-py3-none-any.whl (63 kB)\n",
      "Downloading alive_progress-3.2.0-py3-none-any.whl (77 kB)\n",
      "Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
      "Downloading win32_setctime-1.2.0-py3-none-any.whl (4.1 kB)\n",
      "Building wheels for collected packages: grapheme\n",
      "  Building wheel for grapheme (setup.py): started\n",
      "  Building wheel for grapheme (setup.py): finished with status 'done'\n",
      "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210116 sha256=20fc2696761eac9d0b68b2a9ea214aafcf9c250b1a666cda0d30cf77887da1c6\n",
      "  Stored in directory: c:\\users\\access\\appdata\\local\\pip\\cache\\wheels\\5b\\aa\\3b\\d94434910f5e19ac7f8aa6523d74a46fe06bfcbc7e4b26caf6\n",
      "Successfully built grapheme\n",
      "Installing collected packages: pytz, grapheme, win32-setctime, requests, about-time, loguru, alive-progress, tickterial\n",
      "Successfully installed about-time-4.2.1 alive-progress-3.2.0 grapheme-0.6.0 loguru-0.7.2 pytz-2022.7 requests-2.32.2 tickterial-1.1.2 win32-setctime-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tickterial.exe is installed in 'C:\\Users\\Access\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ctrader-open-api 0.9.2 requires protobuf==3.20.1, but you have protobuf 4.25.7 which is incompatible.\n",
      "ctrader-open-api 0.9.2 requires requests==2.32.3, but you have requests 2.32.2 which is incompatible.\n",
      "streamlit 1.32.0 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tickterial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9303fbbd-935a-4eb7-9704-d772d7014d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'tickterial' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import tickterial\n",
    "!tickterial --symbols XAUUSD --start 2025-01-01 --end 2025-07-10 --progress true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e93280-96fb-4732-b718-4ba2a1cc3ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/femtotrader/dukascopy.git\n",
      "  Cloning https://github.com/femtotrader/dukascopy.git to c:\\users\\access\\appdata\\local\\temp\\pip-req-build-_c2b14m9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/femtotrader/dukascopy.git 'C:\\Users\\Access\\AppData\\Local\\Temp\\pip-req-build-_c2b14m9'\n",
      "  remote: Repository not found.\n",
      "  fatal: repository 'https://github.com/femtotrader/dukascopy.git/' not found\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  git clone --filter=blob:none --quiet https://github.com/femtotrader/dukascopy.git 'C:\\Users\\Access\\AppData\\Local\\Temp\\pip-req-build-_c2b14m9' did not run successfully.\n",
      "  exit code: 128\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "git clone --filter=blob:none --quiet https://github.com/femtotrader/dukascopy.git 'C:\\Users\\Access\\AppData\\Local\\Temp\\pip-req-build-_c2b14m9' did not run successfully.\n",
      "exit code: 128\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/femtotrader/dukascopy.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f634b5b-0c8b-451e-9772-910ecb36467a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dukascopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdukascopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_hist_data\n\u001b[0;32m      2\u001b[0m get_hist_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEURUSD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-01-10\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dukascopy'"
     ]
    }
   ],
   "source": [
    "from dukascopy import get_hist_data\n",
    "get_hist_data(\"EURUSD\", \"1m\", \"2023-01-01\", \"2023-01-10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7634366-0a22-4006-a98f-8f70a980090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1-minute candles for EURUSD from 2024-01-01 to 2024-01-03...\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/00h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/01h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/02h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/03h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/04h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/05h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/06h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/07h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/08h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/09h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/10h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/11h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/12h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/13h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/14h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/15h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/16h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/17h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/18h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/19h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/20h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Error at https://datafeed.dukascopy.com/datafeed/EURUSD/2024/00/01/21h_ticks.bi5 → Compressed data ended before the end-of-stream marker was reached\n",
      "Done! Saved to: eurusd_1m.csv\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import lzma\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_dukascopy_1m(symbol, start_date, end_date, output_file):\n",
    "    symbol = symbol.upper()\n",
    "    base_url = \"https://datafeed.dukascopy.com/datafeed/{}/{}/{:02d}/{:02d}/{:02d}h_ticks.bi5\"\n",
    "\n",
    "    start = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = dt.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    current = start\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    print(f\"Downloading 1-minute candles for {symbol} from {start_date} to {end_date}...\")\n",
    "    while current <= end:\n",
    "        for hour in range(24):\n",
    "            url = base_url.format(symbol, current.year, current.month - 1, current.day, hour)\n",
    "            try:\n",
    "                r = requests.get(url, timeout=10)\n",
    "                if r.status_code != 200:\n",
    "                    continue\n",
    "                data = lzma.decompress(r.content)\n",
    "                for i in range(0, len(data), 20):\n",
    "                    chunk = data[i:i+20]\n",
    "                    if len(chunk) < 20:\n",
    "                        continue\n",
    "                    millis = int.from_bytes(chunk[0:4], 'big')\n",
    "                    o = int.from_bytes(chunk[4:8], 'big', signed=True) / 1e5\n",
    "                    h = int.from_bytes(chunk[8:12], 'big', signed=True) / 1e5\n",
    "                    l = int.from_bytes(chunk[12:16], 'big', signed=True) / 1e5\n",
    "                    c = int.from_bytes(chunk[16:20], 'big', signed=True) / 1e5\n",
    "                    timestamp = dt.datetime(current.year, current.month, current.day, hour) + dt.timedelta(milliseconds=millis)\n",
    "                    all_rows.append([timestamp, o, h, l, c])\n",
    "            except Exception as e:\n",
    "                print(f\"Error at {url} → {e}\")\n",
    "        current += dt.timedelta(days=1)\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=[\"Time\", \"Open\", \"High\", \"Low\", \"Close\"])\n",
    "    df = df.set_index(\"Time\").resample(\"1min\").ohlc().dropna()\n",
    "    df.columns = df.columns.droplevel(0)\n",
    "    df.to_csv(output_file)\n",
    "    print(f\"Done! Saved to: {output_file}\")\n",
    "\n",
    "# استخدام السكربت:\n",
    "download_dukascopy_1m(\"EURUSD\", \"2024-01-01\", \"2024-01-03\", \"eurusd_1m.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642df107-6815-4bad-9251-238fe8af6990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eurusd_2024-01-02_14h.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lzma\n",
    "import requests\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "def download_bi5_and_save_csv(symbol, year, month, day, hour, output_csv):\n",
    "    symbol = symbol.upper()\n",
    "    url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{year}/{month-1:02}/{day:02}/{hour:02}h_ticks.bi5\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download: {url}\")\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        raw = lzma.decompress(r.content)\n",
    "    except lzma.LZMAError:\n",
    "        print(\"Failed to decompress the file (corrupt or empty).\")\n",
    "        return\n",
    "\n",
    "    records = []\n",
    "    for i in range(0, len(raw), 20):\n",
    "        chunk = raw[i:i+20]\n",
    "        if len(chunk) < 20:\n",
    "            continue\n",
    "        millis = int.from_bytes(chunk[0:4], 'big')\n",
    "        o = int.from_bytes(chunk[4:8], 'big', signed=True) / 1e5\n",
    "        h = int.from_bytes(chunk[8:12], 'big', signed=True) / 1e5\n",
    "        l = int.from_bytes(chunk[12:16], 'big', signed=True) / 1e5\n",
    "        c = int.from_bytes(chunk[16:20], 'big', signed=True) / 1e5\n",
    "        timestamp = dt.datetime(year, month, day, hour) + dt.timedelta(milliseconds=millis)\n",
    "        records.append([timestamp, o, h, l, c])\n",
    "\n",
    "    df = pd.DataFrame(records, columns=[\"Time\", \"Open\", \"High\", \"Low\", \"Close\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved: {output_csv}\")\n",
    "\n",
    "\n",
    "# 🔽 Example: Download EURUSD data on Jan 2, 2024 at 14:00 GMT\n",
    "download_bi5_and_save_csv(\"EURUSD\", 2024, 1, 2, 14, \"eurusd_2024-01-02_14h.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053dd40e-8c49-4a41-891e-bfe685e158a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: csv/EURUSD_2024-01-02_14h.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import lzma\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "def download_and_convert_to_csv(symbol, year, month, day, hour, output_folder=\"csv\"):\n",
    "    symbol = symbol.upper()\n",
    "    url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{year}/{month-1:02}/{day:02}/{hour:02}h_ticks.bi5\"\n",
    "\n",
    "    # File paths\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    csv_file = f\"{output_folder}/{symbol}_{year}-{month:02}-{day:02}_{hour:02}h.csv\"\n",
    "\n",
    "    try:\n",
    "        # Download\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        compressed = response.content\n",
    "        # Decompress\n",
    "        data = lzma.decompress(compressed)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading or decompressing: {url}\\n{e}\")\n",
    "        return\n",
    "\n",
    "    # Parse binary into OHLC data\n",
    "    records = []\n",
    "    for i in range(0, len(data), 20):\n",
    "        chunk = data[i:i+20]\n",
    "        if len(chunk) < 20:\n",
    "            continue\n",
    "        millis = int.from_bytes(chunk[0:4], 'big')\n",
    "        o = int.from_bytes(chunk[4:8], 'big', signed=True) / 1e5\n",
    "        h = int.from_bytes(chunk[8:12], 'big', signed=True) / 1e5\n",
    "        l = int.from_bytes(chunk[12:16], 'big', signed=True) / 1e5\n",
    "        c = int.from_bytes(chunk[16:20], 'big', signed=True) / 1e5\n",
    "        timestamp = dt.datetime(year, month, day, hour) + dt.timedelta(milliseconds=millis)\n",
    "        records.append([timestamp, o, h, l, c])\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(records, columns=[\"Time\", \"Open\", \"High\", \"Low\", \"Close\"])\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"✅ Saved: {csv_file}\")\n",
    "\n",
    "# ▶️ Example: EURUSD on 2024-01-02 at 14:00\n",
    "download_and_convert_to_csv(\"EURUSD\", 2024, 1, 2, 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e97c1a8-6d8b-4410-877e-dd7db802129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1-minute candles: csv/EURUSD_2024-01-02_14h_1min.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import lzma\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "def download_and_convert_to_1min_csv(symbol, year, month, day, hour, output_folder=\"csv\"):\n",
    "    symbol = symbol.upper()\n",
    "    url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{year}/{month-1:02}/{day:02}/{hour:02}h_ticks.bi5\"\n",
    "\n",
    "    # File paths\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    csv_file = f\"{output_folder}/{symbol}_{year}-{month:02}-{day:02}_{hour:02}h_1min.csv\"\n",
    "\n",
    "    try:\n",
    "        # Download and decompress\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = lzma.decompress(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Parse each tick\n",
    "    rows = []\n",
    "    for i in range(0, len(data), 20):\n",
    "        chunk = data[i:i+20]\n",
    "        if len(chunk) < 20:\n",
    "            continue\n",
    "        ms = int.from_bytes(chunk[0:4], 'big')\n",
    "        ask_open = int.from_bytes(chunk[4:8], 'big', signed=True) / 1e5\n",
    "        ask_high = int.from_bytes(chunk[8:12], 'big', signed=True) / 1e5\n",
    "        ask_low  = int.from_bytes(chunk[12:16], 'big', signed=True) / 1e5\n",
    "        ask_close= int.from_bytes(chunk[16:20], 'big', signed=True) / 1e5\n",
    "        timestamp = dt.datetime(year, month, day, hour) + dt.timedelta(milliseconds=ms)\n",
    "        rows.append([timestamp, ask_open, ask_high, ask_low, ask_close])\n",
    "\n",
    "    # Convert to DataFrame and resample to 1-minute OHLC\n",
    "    df = pd.DataFrame(rows, columns=[\"Time\", \"Open\", \"High\", \"Low\", \"Close\"])\n",
    "    df = df.set_index(\"Time\").resample(\"1min\").agg({\n",
    "        \"Open\": \"first\",\n",
    "        \"High\": \"max\",\n",
    "        \"Low\": \"min\",\n",
    "        \"Close\": \"last\"\n",
    "    }).dropna()\n",
    "\n",
    "    df.to_csv(csv_file)\n",
    "    print(f\"✅ Saved 1-minute candles: {csv_file}\")\n",
    "\n",
    "# مثال: تحميل ساعة واحدة وتحويلها لشموع دقيقة\n",
    "download_and_convert_to_1min_csv(\"EURUSD\", 2024, 1, 2, 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc79d2a6-fbbe-4c03-a920-c3c79f31d874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BID-based 1-minute candles saved to: csv/EURUSD_2024-01-02_14h_bid_1min.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import lzma\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "def download_bi5_and_convert_to_1min_bid_csv(symbol, year, month, day, hour, output_folder=\"csv\"):\n",
    "    symbol = symbol.upper()\n",
    "    url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{year}/{month-1:02}/{day:02}/{hour:02}h_ticks.bi5\"\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_file = f\"{output_folder}/{symbol}_{year}-{month:02}-{day:02}_{hour:02}h_bid_1min.csv\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        raw = lzma.decompress(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading or decompressing from:\\n{url}\\n→ {e}\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for i in range(0, len(raw), 20):\n",
    "        chunk = raw[i:i+20]\n",
    "        if len(chunk) < 20:\n",
    "            continue\n",
    "        millis     = int.from_bytes(chunk[0:4], 'big')\n",
    "        bid_price  = int.from_bytes(chunk[8:12], 'big', signed=True) / 1e5\n",
    "        timestamp  = dt.datetime(year, month, day, hour) + dt.timedelta(milliseconds=millis)\n",
    "        rows.append([timestamp, bid_price])\n",
    "\n",
    "    if not rows:\n",
    "        print(\"⚠️ No data found.\")\n",
    "        return\n",
    "\n",
    "    # Convert to DataFrame and resample into 1-minute candles\n",
    "    df = pd.DataFrame(rows, columns=[\"Time\", \"Bid\"])\n",
    "    df = df.set_index(\"Time\").resample(\"1min\").agg({\n",
    "        \"Bid\": [\"first\", \"max\", \"min\", \"last\"]\n",
    "    }).dropna()\n",
    "\n",
    "    df.columns = [\"Open\", \"High\", \"Low\", \"Close\"]  # Rename columns\n",
    "    df.to_csv(output_file)\n",
    "    print(f\"✅ BID-based 1-minute candles saved to: {output_file}\")\n",
    "\n",
    "# 🔽 مثال: تحميل شموع 1 دقيقة لزوج EURUSD بتاريخ وساعة محددة\n",
    "download_bi5_and_convert_to_1min_bid_csv(\"EURUSD\", 2024, 1, 2, 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc788e0-eed4-407f-821b-0b27b225d7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
