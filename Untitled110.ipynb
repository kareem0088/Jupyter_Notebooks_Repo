{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb76466d-9200-4e14-9b7d-98924e45a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Ø£Ø¯Ø§Ø© ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Dukascopy (Ø´Ù…ÙˆØ¹ / ØªÙŠÙƒ) Ù…ØªÙ‚Ø¯Ù…Ø©\n",
      "\n",
      "ğŸ”¹ Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Ø§Ø®ØªØ± Ø±Ù‚Ù…:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©\n",
      "1. XAUUSD\n",
      "2. EURUSD\n",
      "3. GBPUSD\n",
      "4. USDJPY\n",
      "5. USDCHF\n",
      "6. AUDUSD\n",
      "7. NZDUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Ø§Ø®ØªØ± Ø±Ù‚Ù…:  1\n",
      "â° Ø£Ø¯Ø®Ù„ GMT offset (Ù…Ø«Ø§Ù„ 0 Ø£Ùˆ 2 Ø£Ùˆ -5) [Ø§ÙØªØ±Ø§Ø¶ÙŠ 0]:  \n",
      "ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ù…Ø«Ø§Ù„: 2020-01-01):  2020-01-01\n",
      "ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Ù…Ø«Ø§Ù„: 2020-01-01):  2020-02-01\n",
      "\n",
      "ğŸ“ Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸ (Enter Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: C:\\\\Users\\\\Access\\\\Documents\\\\DATA):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¹Ø±\n",
      "1. Bid ÙÙ‚Ø·\n",
      "2. Ask ÙÙ‚Ø·\n",
      "3. ÙƒÙ„Ø§Ù‡Ù…Ø§\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Ø§Ø®ØªØ± Ø±Ù‚Ù…:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.00% | Ù…ØªØ¨Ù‚ÙŠ: 0 Ø«Ø«\n",
      "âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "DEFAULT_SAVE_PATH = r\"C:\\\\Users\\\\Access\\\\Documents\\\\DATA\"\n",
    "SYMBOLS = [\"XAUUSD\", \"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"]\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"60\", \"5m\": \"300\", \"15m\": \"900\", \"1h\": \"3600\", \"4h\": \"14400\", \"1d\": \"86400\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"ğŸ”¢ Ø§Ø®ØªØ± Ø±Ù‚Ù…: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Ø±Ù‚Ù… ØºÙŠØ± ØµØ­ÙŠØ­.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ ÙŠØ¬Ø¨ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù….\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (Ù…Ø«Ø§Ù„: 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Ø§Ù„ØªØ§Ø±ÙŠØ® ØºÙŠØ± ØµØ­ÙŠØ­.\")\n",
    "\n",
    "def get_custom_path(default_path):\n",
    "    choice = input(f\"\\nğŸ“ Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸ (Enter Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: {default_path}): \").strip()\n",
    "    return choice if choice else default_path\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"â° Ø£Ø¯Ø®Ù„ GMT offset (Ù…Ø«Ø§Ù„ 0 Ø£Ùˆ 2 Ø£Ùˆ -5) [Ø§ÙØªØ±Ø§Ø¶ÙŠ 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | Ù…ØªØ¨Ù‚ÙŠ: {int(remaining)} Ø«\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, path, price_type, gmt_offset):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=10)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    for i in range(0, len(data), 20):\n",
    "                        chunk = data[i:i+20]\n",
    "                        if len(chunk) < 20: continue\n",
    "                        t_off, ask, bid, ask_vol, bid_vol = struct.unpack(\" >IIfff\", chunk)\n",
    "                        tick_time = day + timedelta(hours=hour, milliseconds=t_off)\n",
    "                        tick_time += timedelta(hours=gmt_offset)\n",
    "                        row = [tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]]\n",
    "                        if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                            row += [round(bid, 5), round(bid_vol, 2)]\n",
    "                        elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                            row += [round(ask, 5), round(ask_vol, 2)]\n",
    "                        else:\n",
    "                            row += [round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                        all_ticks.append(row)\n",
    "            except: pass\n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time)\n",
    "\n",
    "    if all_ticks:\n",
    "        filename = os.path.join(path, f\"{symbol}_tick_merged.csv\")\n",
    "        with open(filename, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"bid\", \"bid_vol\"])\n",
    "            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"ask\", \"ask_vol\"])\n",
    "            else:\n",
    "                writer.writerow([\"time\", \"bid\", \"ask\", \"bid_vol\", \"ask_vol\"])\n",
    "            all_ticks.sort(key=lambda x: x[0])\n",
    "            writer.writerows(all_ticks)\n",
    "        print(f\"\\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù: {filename}\")\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, start, end, path, gmt_offset):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    url_base = \"https://datafeed.dukascopy.com/datafeed/{}/{}/{}_candles_min_{}_bi5\"\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        year, month, day_ = day.year, day.month - 1, day.day\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{year}/{month:02d}/{day_:02d}/\" \\\n",
    "              f\"{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                data = lzma.decompress(r.content)\n",
    "                for i in range(0, len(data), 20):\n",
    "                    chunk = data[i:i+20]\n",
    "                    if len(chunk) < 20: continue\n",
    "                    utc_offset, open_, high, low, close, vol = struct.unpack(\n",
    "                        \">IIffff\", chunk[:20])\n",
    "                    candle_time = day + timedelta(seconds=utc_offset) + timedelta(hours=gmt_offset)\n",
    "                    row = [candle_time.strftime(\"%Y-%m-%d %H:%M:%S\"), round(open_, 5), round(high, 5),\n",
    "                           round(low, 5), round(close, 5), round(vol, 2)]\n",
    "                    all_data.append(row)\n",
    "        except: pass\n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time)\n",
    "\n",
    "    if all_data:\n",
    "        filename = os.path.join(path, f\"{symbol}_candles_{tf_code}_merged.csv\")\n",
    "        with open(filename, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "            all_data.sort(key=lambda x: x[0])\n",
    "            writer.writerows(all_data)\n",
    "        print(f\"\\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù: {filename}\")\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Ø£Ø¯Ø§Ø© ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Dukascopy (Ø´Ù…ÙˆØ¹ / ØªÙŠÙƒ) Ù…ØªÙ‚Ø¯Ù…Ø©\")\n",
    "\n",
    "    data_type = show_menu([\"Tick\", \"Candlestick\"], \"Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\")\n",
    "    symbol = show_menu(SYMBOLS, \"Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©\")\n",
    "    symbol = SYMBOLS.index(symbol)\n",
    "    gmt_offset = get_gmt_offset()\n",
    "    start = get_date_input(\"ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©\")\n",
    "    end = get_date_input(\"ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ©\")\n",
    "    save_path = get_custom_path(DEFAULT_SAVE_PATH)\n",
    "\n",
    "    if start > end:\n",
    "        print(\"âŒ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨Ø¹Ø¯ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©.\")\n",
    "        exit()\n",
    "\n",
    "    if data_type == \"Tick\":\n",
    "        price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¹Ø±\")\n",
    "        download_tick(SYMBOLS[symbol], start, end, save_path, price_type, gmt_offset)\n",
    "\n",
    "    else:\n",
    "        tf = show_menu(TIMEFRAMES, \"Ø§Ø®ØªØ± Ø§Ù„ØªØ§ÙŠÙ… ÙØ±ÙŠÙ…\")\n",
    "        tf_code = CANDLE_FRAME_CODES[tf]\n",
    "        download_candles(SYMBOLS[symbol], tf_code, start, end, save_path, gmt_offset)\n",
    "\n",
    "    print(\"\\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e93ba8-9aa0-4714-9c6c-c2d143ac293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool\n",
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2020-01-01\n",
      "End date (e.g. 2020-01-01):  2020-01-03\n",
      "\n",
      "ğŸ“ Save path (Enter to use default: C:\\\\Users\\\\Access\\\\Documents\\\\DATA):  C:\\\\Users\\\\Access\\\\Documents\\\\DATA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Price type\n",
      "1. Bid ÙÙ‚Ø·\n",
      "2. Ask ÙÙ‚Ø·\n",
      "3. ÙƒÙ„Ø§Ù‡Ù…Ø§\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.00% | 0.8MB of 2.1MB | ETA: 0h 0m 0s\n",
      "âœ… Download completed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "DEFAULT_SAVE_PATH = r\"C:\\\\Users\\\\Access\\\\Documents\\\\DATA\"\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"60\", \"5m\": \"300\", \"15m\": \"900\", \"1h\": \"3600\", \"4h\": \"14400\", \"1d\": \"86400\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_custom_path(default_path):\n",
    "    choice = input(f\"\\nğŸ“ Save path (Enter to use default: {default_path}): \").strip()\n",
    "    return choice if choice else default_path\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def format_seconds(seconds):\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØµÙŠØºØ© (Ø³Ø§Ø¹Ø§Øª - Ø¯Ù‚Ø§Ø¦Ù‚ - Ø«ÙˆØ§Ù†ÙŠ)\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, path, price_type, gmt_offset):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024  # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ø­Ø¬Ù…\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=10)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    for i in range(0, len(data), 20):\n",
    "                        chunk = data[i:i+20]\n",
    "                        if len(chunk) < 20: continue\n",
    "                        t_off, ask, bid, ask_vol, bid_vol = struct.unpack(\" >IIfff\", chunk)\n",
    "                        tick_time = day + timedelta(hours=hour, milliseconds=t_off)\n",
    "                        tick_time += timedelta(hours=gmt_offset)\n",
    "                        row = [tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]]\n",
    "                        if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                            row += [round(bid, 5), round(bid_vol, 2)]\n",
    "                        elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                            row += [round(ask, 5), round(ask_vol, 2)]\n",
    "                        else:\n",
    "                            row += [round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                        all_ticks.append(row)\n",
    "            except: pass\n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    if all_ticks:\n",
    "        filename = os.path.join(path, f\"{symbol}_tick_merged.csv\")\n",
    "        with open(filename, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"bid\", \"bid_vol\"])\n",
    "            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"ask\", \"ask_vol\"])\n",
    "            else:\n",
    "                writer.writerow([\"time\", \"bid\", \"ask\", \"bid_vol\", \"ask_vol\"])\n",
    "            all_ticks.sort(key=lambda x: x[0])\n",
    "            writer.writerows(all_ticks)\n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… File saved: {filename}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, start, end, path, gmt_offset):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024  # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ø­Ø¬Ù…\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                downloaded_bytes += len(r.content)\n",
    "                data = lzma.decompress(r.content)\n",
    "                for i in range(0, len(data), 20):\n",
    "                    chunk = data[i:i+20]\n",
    "                    if len(chunk) < 20: continue\n",
    "                    utc_offset, open_, high, low, close, vol = struct.unpack(\n",
    "                        \">IIffff\", chunk[:20])\n",
    "                    candle_time = day + timedelta(seconds=utc_offset) + timedelta(hours=gmt_offset)\n",
    "                    row = [candle_time.strftime(\"%Y-%m-%d %H:%M:%S\"), round(open_, 5), round(high, 5),\n",
    "                           round(low, 5), round(close, 5), round(vol, 2)]\n",
    "                    all_data.append(row)\n",
    "        except: pass\n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    if all_data:\n",
    "        filename = os.path.join(path, f\"{symbol}_candles_{tf_code}_merged.csv\")\n",
    "        with open(filename, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "            all_data.sort(key=lambda x: x[0])\n",
    "            writer.writerows(all_data)\n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… File saved: {filename}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool\")\n",
    "\n",
    "    data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "    category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "    symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "    gmt_offset = get_gmt_offset()\n",
    "    start = get_date_input(\"Start date\")\n",
    "    end = get_date_input(\"End date\")\n",
    "    save_path = get_custom_path(DEFAULT_SAVE_PATH)\n",
    "\n",
    "    if start > end:\n",
    "        print(\"âŒ End date must be after start date.\")\n",
    "        exit()\n",
    "\n",
    "    if data_type == \"Tick\":\n",
    "        price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "        download_tick(symbol, start, end, save_path, price_type, gmt_offset)\n",
    "    else:\n",
    "        tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "        tf_code = CANDLE_FRAME_CODES[tf]\n",
    "        download_candles(symbol, tf_code, start, end, save_path, gmt_offset)\n",
    "\n",
    "    print(\"\\nâœ… Download completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fe35f4-a77e-479b-bf7f-1142911f5374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool\n",
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2020-01-02\n",
      "End date (e.g. 2020-01-01):  2020-01-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Price type\n",
      "1. Bid ÙÙ‚Ø·\n",
      "2. Ask ÙÙ‚Ø·\n",
      "3. ÙƒÙ„Ø§Ù‡Ù…Ø§\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.00% | 0.8MB of 1.4MB | ETA: 0h 0m 0s\n",
      "âœ… Download completed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "DEFAULT_SAVE_PATH = os.getcwd()\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"60\", \"5m\": \"300\", \"15m\": \"900\", \"1h\": \"3600\", \"4h\": \"14400\", \"1d\": \"86400\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def format_seconds(seconds):\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØµÙŠØºØ© (Ø³Ø§Ø¹Ø§Øª - Ø¯Ù‚Ø§Ø¦Ù‚ - Ø«ÙˆØ§Ù†ÙŠ)\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, price_type, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024  # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ø­Ø¬Ù…\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=10)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    for i in range(0, len(data), 20):\n",
    "                        chunk = data[i:i+20]\n",
    "                        if len(chunk) < 20: continue\n",
    "                        t_off, ask, bid, ask_vol, bid_vol = struct.unpack(\" >IIfff\", chunk)\n",
    "                        tick_time = day + timedelta(hours=hour, milliseconds=t_off)\n",
    "                        tick_time += timedelta(hours=gmt_offset)\n",
    "                        row = [tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]]\n",
    "                        if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                            row += [round(bid, 5), round(bid_vol, 2)]\n",
    "                        elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                            row += [round(ask, 5), round(ask_vol, 2)]\n",
    "                        else:\n",
    "                            row += [round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                        all_ticks.append(row)\n",
    "            except: pass\n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    if all_ticks:\n",
    "        filename = os.path.join(path, f\"{symbol}_tick_merged.csv\")\n",
    "        with open(filename, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"bid\", \"bid_vol\"])\n",
    "            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"ask\", \"ask_vol\"])\n",
    "            else:\n",
    "                writer.writerow([\"time\", \"bid\", \"ask\", \"bid_vol\", \"ask_vol\"])\n",
    "            all_ticks.sort(key=lambda x: x[0])\n",
    "            writer.writerows(all_ticks)\n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, start, end, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024  # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ø­Ø¬Ù…\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                downloaded_bytes += len(r.content)\n",
    "                data = lzma.decompress(r.content)\n",
    "                for i in range(0, len(data), 20):\n",
    "                    chunk = data[i:i+20]\n",
    "                    if len(chunk) < 20: continue\n",
    "                    utc_offset, open_, high, low, close, vol = struct.unpack(\n",
    "                        \">IIffff\", chunk[:20])\n",
    "                    candle_time = day + timedelta(seconds=utc_offset) + timedelta(hours=gmt_offset)\n",
    "                    row = [candle_time.strftime(\"%Y-%m-%d %H:%M:%S\"), round(open_, 5), round(high, 5),\n",
    "                           round(low, 5), round(close, 5), round(vol, 2)]\n",
    "                    all_data.append(row)\n",
    "        except: pass\n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    if all_data:\n",
    "        filename = os.path.join(path, f\"{symbol}_candles_{tf_code}_merged.csv\")\n",
    "        with open(filename, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "            all_data.sort(key=lambda x: x[0])\n",
    "            writer.writerows(all_data)\n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool\")\n",
    "\n",
    "    data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "    category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "    symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "    gmt_offset = get_gmt_offset()\n",
    "    start = get_date_input(\"Start date\")\n",
    "    end = get_date_input(\"End date\")\n",
    "\n",
    "    if start > end:\n",
    "        print(\"âŒ End date must be after start date.\")\n",
    "        exit()\n",
    "\n",
    "    if data_type == \"Tick\":\n",
    "        price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "        download_tick(symbol, start, end, price_type, gmt_offset)\n",
    "    else:\n",
    "        tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "        tf_code = CANDLE_FRAME_CODES[tf]\n",
    "        download_candles(symbol, tf_code, start, end, gmt_offset)\n",
    "\n",
    "    print(\"\\nâœ… Download completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e79c0185-8592-4e1b-824d-038729d46220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool\n",
      "========================================\n",
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2020-01-05\n",
      "End date (e.g. 2020-01-01):  2020-01-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Configuration Summary:\n",
      "   - Symbol: XAUUSD\n",
      "   - Data type: Tick\n",
      "   - Date range: 2020-01-05 to 2020-01-07\n",
      "   - GMT offset: 0\n",
      "   - Save path: C:\\Users\\Access\n",
      "\n",
      "ğŸ”¹ Price type\n",
      "1. Bid ÙÙ‚Ø·\n",
      "2. Ask ÙÙ‚Ø·\n",
      "3. ÙƒÙ„Ø§Ù‡Ù…Ø§\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Price type: ÙƒÙ„Ø§Ù‡Ù…Ø§\n",
      "\n",
      "ğŸ”„ Starting tick download for XAUUSD...\n",
      "ğŸ“… Date range: 2020-01-05 to 2020-01-07\n",
      "â° GMT offset: 0\n",
      "[======                                            ] 13.89% | 0.0MB of 2.1MB | ETA: 0h 0m 38s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 10h\n",
      "[=======                                           ] 15.28% | 0.0MB of 2.1MB | ETA: 0h 0m 36s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 11h\n",
      "[========                                          ] 16.67% | 0.0MB of 2.1MB | ETA: 0h 0m 36s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 12h\n",
      "[=========                                         ] 18.06% | 0.0MB of 2.1MB | ETA: 0h 0m 34s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 13h\n",
      "[=========                                         ] 19.44% | 0.0MB of 2.1MB | ETA: 0h 0m 32s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 14h\n",
      "[==========                                        ] 20.83% | 0.0MB of 2.1MB | ETA: 0h 0m 30s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 15h\n",
      "[===========                                       ] 22.22% | 0.0MB of 2.1MB | ETA: 0h 0m 29s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 16h\n",
      "[===========                                       ] 23.61% | 0.0MB of 2.1MB | ETA: 0h 0m 28s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 17h\n",
      "[============                                      ] 25.00% | 0.0MB of 2.1MB | ETA: 0h 0m 27s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 18h\n",
      "[=============                                     ] 26.39% | 0.0MB of 2.1MB | ETA: 0h 0m 26s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 19h\n",
      "[=============                                     ] 27.78% | 0.0MB of 2.1MB | ETA: 0h 0m 25s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 20h\n",
      "[==============                                    ] 29.17% | 0.0MB of 2.1MB | ETA: 0h 0m 24s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 21h\n",
      "[===============                                   ] 30.56% | 0.0MB of 2.1MB | ETA: 0h 0m 23s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-05 22h\n",
      "[===============================                   ] 63.89% | 0.5MB of 2.1MB | ETA: 0h 0m 17s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-06 22h\n",
      "[================================================  ] 97.22% | 1.0MB of 2.1MB | ETA: 0h 0m 1ss\n",
      "âš ï¸ Warning: HTTP 200 for 2020-01-07 22h\n",
      "[==================================================] 100.00% | 1.0MB of 2.1MB | ETA: 0h 0m 0s\n",
      "\n",
      "ğŸ“Š Download Summary:\n",
      "   - Total hours processed: 72\n",
      "   - Successful downloads: 27\n",
      "   - Total ticks collected: 279519\n",
      "âœ… File saved: C:\\Users\\Access\\XAUUSD_tick_merged.csv\n",
      "ğŸ“¦ Size: 12.00 MB\n",
      "\n",
      "âœ… Download completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "DEFAULT_SAVE_PATH = os.getcwd()\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"60\", \"5m\": \"300\", \"15m\": \"900\", \"1h\": \"3600\", \"4h\": \"14400\", \"1d\": \"86400\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def format_seconds(seconds):\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØµÙŠØºØ© (Ø³Ø§Ø¹Ø§Øª - Ø¯Ù‚Ø§Ø¦Ù‚ - Ø«ÙˆØ§Ù†ÙŠ)\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, price_type, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting tick download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    \n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=15)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    successful_downloads += 1\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    try:\n",
    "                        data = lzma.decompress(r.content)\n",
    "                        for i in range(0, len(data), 20):\n",
    "                            chunk = data[i:i+20]\n",
    "                            if len(chunk) < 20: \n",
    "                                continue\n",
    "                            t_off, ask, bid, ask_vol, bid_vol = struct.unpack(\">IIfff\", chunk)\n",
    "                            tick_time = day + timedelta(hours=hour, milliseconds=t_off)\n",
    "                            tick_time += timedelta(hours=gmt_offset)\n",
    "                            row = [tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]]\n",
    "                            \n",
    "                            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                                row += [round(bid, 5), round(bid_vol, 2)]\n",
    "                            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                                row += [round(ask, 5), round(ask_vol, 2)]\n",
    "                            else:\n",
    "                                row += [round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                            all_ticks.append(row)\n",
    "                    except lzma.LZMAError as e:\n",
    "                        print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                        continue\n",
    "                elif r.status_code == 404:\n",
    "                    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®/Ø§Ù„Ø³Ø§Ø¹Ø© (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')} {hour}h\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                continue\n",
    "                \n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total hours processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total ticks collected: {len(all_ticks)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø©\n",
    "    filename = os.path.join(path, f\"{symbol}_tick_merged.csv\")\n",
    "    try:\n",
    "        with open(filename, \"w\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"bid\", \"bid_vol\"])\n",
    "            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"ask\", \"ask_vol\"])\n",
    "            else:\n",
    "                writer.writerow([\"time\", \"bid\", \"ask\", \"bid_vol\", \"ask_vol\"])\n",
    "            \n",
    "            if all_ticks:\n",
    "                all_ticks.sort(key=lambda x: x[0])\n",
    "                writer.writerows(all_ticks)\n",
    "            else:\n",
    "                print(\"âš ï¸ No tick data was collected. Creating empty file with headers only.\")\n",
    "        \n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, start, end, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting candle download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    print(f\"ğŸ“Š Timeframe code: {tf_code}\")\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                successful_downloads += 1\n",
    "                downloaded_bytes += len(r.content)\n",
    "                try:\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    for i in range(0, len(data), 24):  # Changed from 20 to 24 bytes for candles\n",
    "                        chunk = data[i:i+24]\n",
    "                        if len(chunk) < 24: \n",
    "                            continue\n",
    "                        utc_offset, open_, high, low, close, vol = struct.unpack(\">IIffff\", chunk[:24])\n",
    "                        candle_time = day + timedelta(seconds=utc_offset) + timedelta(hours=gmt_offset)\n",
    "                        row = [candle_time.strftime(\"%Y-%m-%d %H:%M:%S\"), round(open_, 5), round(high, 5),\n",
    "                               round(low, 5), round(close, 5), round(vol, 2)]\n",
    "                        all_data.append(row)\n",
    "                except lzma.LZMAError as e:\n",
    "                    print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "                    continue\n",
    "            elif r.status_code == 404:\n",
    "                # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total days processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total candles collected: {len(all_data)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø©\n",
    "    filename = os.path.join(path, f\"{symbol}_candles_{tf_code}_merged.csv\")\n",
    "    try:\n",
    "        with open(filename, \"w\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "            \n",
    "            if all_data:\n",
    "                all_data.sort(key=lambda x: x[0])\n",
    "                writer.writerows(all_data)\n",
    "            else:\n",
    "                print(\"âš ï¸ No candle data was collected. Creating empty file with headers only.\")\n",
    "        \n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    try:\n",
    "        data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "        category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "        symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "        gmt_offset = get_gmt_offset()\n",
    "        start = get_date_input(\"Start date\")\n",
    "        end = get_date_input(\"End date\")\n",
    "\n",
    "        if start > end:\n",
    "            print(\"âŒ End date must be after start date.\")\n",
    "            exit()\n",
    "\n",
    "        print(f\"\\nğŸ“ Configuration Summary:\")\n",
    "        print(f\"   - Symbol: {symbol}\")\n",
    "        print(f\"   - Data type: {data_type}\")\n",
    "        print(f\"   - Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   - GMT offset: {gmt_offset}\")\n",
    "        print(f\"   - Save path: {DEFAULT_SAVE_PATH}\")\n",
    "\n",
    "        success = False\n",
    "        if data_type == \"Tick\":\n",
    "            price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "            print(f\"   - Price type: {price_type}\")\n",
    "            success = download_tick(symbol, start, end, price_type, gmt_offset)\n",
    "        else:\n",
    "            tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "            tf_code = CANDLE_FRAME_CODES[tf]\n",
    "            print(f\"   - Timeframe: {tf}\")\n",
    "            success = download_candles(symbol, tf_code, start, end, gmt_offset)\n",
    "\n",
    "        if success:\n",
    "            print(\"\\nâœ… Download completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Download completed with errors!\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâ¹ï¸ Download interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c579db6f-2bc4-484e-a5a2-3a2d0fa7bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool\n",
      "========================================\n",
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2020-02-01\n",
      "End date (e.g. 2020-01-01):  2020-02-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Configuration Summary:\n",
      "   - Symbol: XAUUSD\n",
      "   - Data type: Tick\n",
      "   - Date range: 2020-02-01 to 2020-02-03\n",
      "   - GMT offset: 0\n",
      "   - Save path: C:\\Users\\Access\n",
      "\n",
      "ğŸ”¹ Price type\n",
      "1. Bid ÙÙ‚Ø·\n",
      "2. Ask ÙÙ‚Ø·\n",
      "3. ÙƒÙ„Ø§Ù‡Ù…Ø§\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Price type: ÙƒÙ„Ø§Ù‡Ù…Ø§\n",
      "\n",
      "ğŸ”„ Starting tick download for XAUUSD...\n",
      "ğŸ“… Date range: 2020-02-01 to 2020-02-03\n",
      "â° GMT offset: 0\n",
      "[======                                            ] 13.89% | 0.0MB of 2.1MB | ETA: 0h 0m 33s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 10h\n",
      "[=======                                           ] 15.28% | 0.0MB of 2.1MB | ETA: 0h 0m 31s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 11h\n",
      "[========                                          ] 16.67% | 0.0MB of 2.1MB | ETA: 0h 0m 29s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 12h\n",
      "[=========                                         ] 18.06% | 0.0MB of 2.1MB | ETA: 0h 0m 27s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 13h\n",
      "[=========                                         ] 19.44% | 0.0MB of 2.1MB | ETA: 0h 0m 25s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 14h\n",
      "[==========                                        ] 20.83% | 0.0MB of 2.1MB | ETA: 0h 0m 24s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 15h\n",
      "[===========                                       ] 22.22% | 0.0MB of 2.1MB | ETA: 0h 0m 23s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 16h\n",
      "[===========                                       ] 23.61% | 0.0MB of 2.1MB | ETA: 0h 0m 22s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 17h\n",
      "[============                                      ] 25.00% | 0.0MB of 2.1MB | ETA: 0h 0m 21s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 18h\n",
      "[=============                                     ] 26.39% | 0.0MB of 2.1MB | ETA: 0h 0m 20s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 19h\n",
      "[=============                                     ] 27.78% | 0.0MB of 2.1MB | ETA: 0h 0m 19s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 20h\n",
      "[==============                                    ] 29.17% | 0.0MB of 2.1MB | ETA: 0h 0m 19s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 21h\n",
      "[===============                                   ] 30.56% | 0.0MB of 2.1MB | ETA: 0h 0m 18s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 22h\n",
      "[===============                                   ] 31.94% | 0.0MB of 2.1MB | ETA: 0h 0m 17s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-01 23h\n",
      "[=======================                           ] 47.22% | 0.0MB of 2.1MB | ETA: 0h 0m 13s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 10h\n",
      "[========================                          ] 48.61% | 0.0MB of 2.1MB | ETA: 0h 0m 12s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 11h\n",
      "[=========================                         ] 50.00% | 0.0MB of 2.1MB | ETA: 0h 0m 12s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 12h\n",
      "[=========================                         ] 51.39% | 0.0MB of 2.1MB | ETA: 0h 0m 11s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 13h\n",
      "[==========================                        ] 52.78% | 0.0MB of 2.1MB | ETA: 0h 0m 11s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 14h\n",
      "[===========================                       ] 54.17% | 0.0MB of 2.1MB | ETA: 0h 0m 10s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 15h\n",
      "[===========================                       ] 55.56% | 0.0MB of 2.1MB | ETA: 0h 0m 10s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 16h\n",
      "[============================                      ] 56.94% | 0.0MB of 2.1MB | ETA: 0h 0m 10s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 17h\n",
      "[=============================                     ] 58.33% | 0.0MB of 2.1MB | ETA: 0h 0m 9s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 18h\n",
      "[=============================                     ] 59.72% | 0.0MB of 2.1MB | ETA: 0h 0m 9s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 19h\n",
      "[==============================                    ] 61.11% | 0.0MB of 2.1MB | ETA: 0h 0m 9s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 20h\n",
      "[===============================                   ] 62.50% | 0.0MB of 2.1MB | ETA: 0h 0m 8s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 21h\n",
      "[===============================                   ] 63.89% | 0.0MB of 2.1MB | ETA: 0h 0m 8s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-02 22h\n",
      "[================================================  ] 97.22% | 0.4MB of 2.1MB | ETA: 0h 0m 0s\n",
      "âš ï¸ Warning: HTTP 200 for 2020-02-03 22h\n",
      "[==================================================] 100.00% | 0.4MB of 2.1MB | ETA: 0h 0m 0s\n",
      "\n",
      "ğŸ“Š Download Summary:\n",
      "   - Total hours processed: 72\n",
      "   - Successful downloads: 14\n",
      "   - Total ticks collected: 111101\n",
      "âŒ Error saving file: [Errno 13] Permission denied: 'C:\\\\Users\\\\Access\\\\XAUUSD_tick_merged.csv'\n",
      "\n",
      "âŒ Download completed with errors!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "DEFAULT_SAVE_PATH = os.getcwd()\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"60\", \"5m\": \"300\", \"15m\": \"900\", \"1h\": \"3600\", \"4h\": \"14400\", \"1d\": \"86400\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def format_seconds(seconds):\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØµÙŠØºØ© (Ø³Ø§Ø¹Ø§Øª - Ø¯Ù‚Ø§Ø¦Ù‚ - Ø«ÙˆØ§Ù†ÙŠ)\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, price_type, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting tick download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    \n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=15)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    successful_downloads += 1\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    try:\n",
    "                        data = lzma.decompress(r.content)\n",
    "                        for i in range(0, len(data), 20):\n",
    "                            chunk = data[i:i+20]\n",
    "                            if len(chunk) < 20: \n",
    "                                continue\n",
    "                            # ÙÙƒ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ time_offset, ask, bid, ask_volume, bid_volume\n",
    "                            t_off, ask, bid, ask_vol, bid_vol = struct.unpack(\">IIfff\", chunk)\n",
    "                            \n",
    "                            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØµØ­ÙŠØ­\n",
    "                            tick_time = day + timedelta(hours=hour, milliseconds=t_off)\n",
    "                            tick_time += timedelta(hours=gmt_offset)\n",
    "                            \n",
    "                            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ù…Ø¹Ø§Ù‹\n",
    "                            formatted_time = tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                            \n",
    "                            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                                row = [formatted_time, round(bid, 5), round(bid_vol, 2)]\n",
    "                            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                                row = [formatted_time, round(ask, 5), round(ask_vol, 2)]\n",
    "                            else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§ - Ø¯Ù…Ø¬ Bid Ùˆ Ask\n",
    "                                row = [formatted_time, round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                            \n",
    "                            all_ticks.append(row)\n",
    "                    except lzma.LZMAError as e:\n",
    "                        print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                        continue\n",
    "                elif r.status_code == 404:\n",
    "                    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®/Ø§Ù„Ø³Ø§Ø¹Ø© (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')} {hour}h\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                continue\n",
    "                \n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total hours processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total ticks collected: {len(all_ticks)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
    "    filename = os.path.join(path, f\"{symbol}_tick_merged.csv\")\n",
    "    try:\n",
    "        with open(filename, \"w\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # ÙƒØªØ§Ø¨Ø© Ø±Ø¤ÙˆØ³ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±\n",
    "            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"datetime\", \"bid\", \"bid_volume\"])\n",
    "            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"datetime\", \"ask\", \"ask_volume\"])\n",
    "            else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§\n",
    "                writer.writerow([\"datetime\", \"bid\", \"ask\", \"bid_volume\", \"ask_volume\"])\n",
    "            \n",
    "            if all_ticks:\n",
    "                # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª\n",
    "                all_ticks.sort(key=lambda x: x[0])\n",
    "                writer.writerows(all_ticks)\n",
    "            else:\n",
    "                print(\"âš ï¸ No tick data was collected. Creating empty file with headers only.\")\n",
    "        \n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, start, end, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting candle download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    print(f\"ğŸ“Š Timeframe code: {tf_code}\")\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                successful_downloads += 1\n",
    "                downloaded_bytes += len(r.content)\n",
    "                try:\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    for i in range(0, len(data), 24):  # Changed from 20 to 24 bytes for candles\n",
    "                        chunk = data[i:i+24]\n",
    "                        if len(chunk) < 24: \n",
    "                            continue\n",
    "                        # ÙÙƒ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø´Ù…ÙˆØ¹\n",
    "                        utc_offset, open_, high, low, close, vol = struct.unpack(\">IIffff\", chunk[:24])\n",
    "                        \n",
    "                        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø´Ù…Ø¹Ø©\n",
    "                        candle_time = day + timedelta(seconds=utc_offset) + timedelta(hours=gmt_offset)\n",
    "                        \n",
    "                        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ù…Ø¹Ø§Ù‹\n",
    "                        formatted_time = candle_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        \n",
    "                        row = [formatted_time, round(open_, 5), round(high, 5),\n",
    "                               round(low, 5), round(close, 5), round(vol, 2)]\n",
    "                        all_data.append(row)\n",
    "                except lzma.LZMAError as e:\n",
    "                    print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "                    continue\n",
    "            elif r.status_code == 404:\n",
    "                # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total days processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total candles collected: {len(all_data)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø©\n",
    "    filename = os.path.join(path, f\"{symbol}_candles_{tf_code}_merged.csv\")\n",
    "    try:\n",
    "        with open(filename, \"w\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # ÙƒØªØ§Ø¨Ø© Ø±Ø¤ÙˆØ³ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„Ù„Ø´Ù…ÙˆØ¹\n",
    "            writer.writerow([\"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "            \n",
    "            if all_data:\n",
    "                # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª\n",
    "                all_data.sort(key=lambda x: x[0])\n",
    "                writer.writerows(all_data)\n",
    "            else:\n",
    "                print(\"âš ï¸ No candle data was collected. Creating empty file with headers only.\")\n",
    "        \n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    try:\n",
    "        data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "        category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "        symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "        gmt_offset = get_gmt_offset()\n",
    "        start = get_date_input(\"Start date\")\n",
    "        end = get_date_input(\"End date\")\n",
    "\n",
    "        if start > end:\n",
    "            print(\"âŒ End date must be after start date.\")\n",
    "            exit()\n",
    "\n",
    "        print(f\"\\nğŸ“ Configuration Summary:\")\n",
    "        print(f\"   - Symbol: {symbol}\")\n",
    "        print(f\"   - Data type: {data_type}\")\n",
    "        print(f\"   - Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   - GMT offset: {gmt_offset}\")\n",
    "        print(f\"   - Save path: {DEFAULT_SAVE_PATH}\")\n",
    "\n",
    "        success = False\n",
    "        if data_type == \"Tick\":\n",
    "            price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "            print(f\"   - Price type: {price_type}\")\n",
    "            success = download_tick(symbol, start, end, price_type, gmt_offset)\n",
    "        else:\n",
    "            tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "            tf_code = CANDLE_FRAME_CODES[tf]\n",
    "            print(f\"   - Timeframe: {tf}\")\n",
    "            success = download_candles(symbol, tf_code, start, end, gmt_offset)\n",
    "\n",
    "        if success:\n",
    "            print(\"\\nâœ… Download completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Download completed with errors!\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâ¹ï¸ Download interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0293aa6-3d3f-4cec-92d1-f224551ee331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool\n",
      "========================================\n",
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Invalid date format.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Start date (e.g. 2020-01-01):  2025-01-01\n",
      "End date (e.g. 2020-01-01):  2025-01-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Configuration Summary:\n",
      "   - Symbol: XAUUSD\n",
      "   - Data type: Candlestick\n",
      "   - Date range: 2025-01-01 to 2025-01-01\n",
      "   - GMT offset: 0\n",
      "   - Save path: C:\\Users\\Access\\Downloads\n",
      "\n",
      "ğŸ”¹ Select timeframe\n",
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Timeframe: 1h\n",
      "\n",
      "ğŸ”„ Starting candle download for XAUUSD...\n",
      "ğŸ“… Date range: 2025-01-01 to 2025-01-01\n",
      "â° GMT offset: 0\n",
      "ğŸ“Š Timeframe code: 3600\n",
      "[==================================================] 100.00% | 0.0MB of 0.0MB | ETA: 0h 0m 0s\n",
      "\n",
      "ğŸ“Š Download Summary:\n",
      "   - Total days processed: 1\n",
      "   - Successful downloads: 0\n",
      "   - Total candles collected: 0\n",
      "âœ… File saved: C:\\Users\\Access\\Downloads\\XAUUSD_candles_3600_merged_1.csv\n",
      "ğŸ“¦ Size: 0.00 MB\n",
      "âš ï¸ No candle data was collected. File contains headers only.\n",
      "\n",
      "âœ… Download completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "def get_save_path():\n",
    "    \"\"\"ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ø¢Ù…Ù†\"\"\"\n",
    "    possible_paths = [\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Downloads\"),  # Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Documents\"),  # Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
    "        os.path.expanduser(\"~\"),  # Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…\n",
    "        os.getcwd()  # Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            # Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙƒØªØ§Ø¨Ø©\n",
    "            test_file = os.path.join(path, \"test_write.tmp\")\n",
    "            with open(test_file, \"w\") as f:\n",
    "                f.write(\"test\")\n",
    "            os.remove(test_file)\n",
    "            return path\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return os.getcwd()  # Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙƒØ®ÙŠØ§Ø± Ø£Ø®ÙŠØ±\n",
    "\n",
    "DEFAULT_SAVE_PATH = get_save_path()\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"60\", \"5m\": \"300\", \"15m\": \"900\", \"1h\": \"3600\", \"4h\": \"14400\", \"1d\": \"86400\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_safe_filename(base_path, filename):\n",
    "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù† Ù…Ø¹ ØªØ¬Ù†Ø¨ ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡\"\"\"\n",
    "    full_path = os.path.join(base_path, filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        return full_path\n",
    "    \n",
    "    # ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø¶Ø§ÙØ© Ø±Ù‚Ù… Ù…ØªØ³Ù„Ø³Ù„\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(base_path, f\"{name}_{counter}{ext}\")):\n",
    "        counter += 1\n",
    "    return os.path.join(base_path, f\"{name}_{counter}{ext}\")\n",
    "\n",
    "def safe_file_write(filename, headers, data, encoding='utf-8'):\n",
    "    \"\"\"ÙƒØªØ§Ø¨Ø© Ø¢Ù…Ù†Ø© Ù„Ù„Ù…Ù„Ù Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª\"\"\"\n",
    "    try:\n",
    "        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯\n",
    "        with open(filename, \"w\", newline='', encoding=encoding) as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(headers)\n",
    "            if data:\n",
    "                writer.writerows(data)\n",
    "        return filename, True, None\n",
    "        \n",
    "    except PermissionError:\n",
    "        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª\n",
    "        downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "        try:\n",
    "            os.makedirs(downloads_path, exist_ok=True)\n",
    "            alt_filename = os.path.join(downloads_path, os.path.basename(filename))\n",
    "            alt_filename = get_safe_filename(downloads_path, os.path.basename(filename))\n",
    "            \n",
    "            with open(alt_filename, \"w\", newline='', encoding=encoding) as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(headers)\n",
    "                if data:\n",
    "                    writer.writerows(data)\n",
    "            return alt_filename, True, \"Saved to Downloads folder due to permission issue\"\n",
    "            \n",
    "        except Exception as e2:\n",
    "            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\n",
    "            try:\n",
    "                desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "                os.makedirs(desktop_path, exist_ok=True)\n",
    "                desktop_filename = os.path.join(desktop_path, os.path.basename(filename))\n",
    "                desktop_filename = get_safe_filename(desktop_path, os.path.basename(filename))\n",
    "                \n",
    "                with open(desktop_filename, \"w\", newline='', encoding=encoding) as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(headers)\n",
    "                    if data:\n",
    "                        writer.writerows(data)\n",
    "                return desktop_filename, True, \"Saved to Desktop due to permission issue\"\n",
    "                \n",
    "            except Exception as e3:\n",
    "                return filename, False, f\"Failed to save file: {str(e3)}\"\n",
    "                \n",
    "def format_seconds(seconds):\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØµÙŠØºØ© (Ø³Ø§Ø¹Ø§Øª - Ø¯Ù‚Ø§Ø¦Ù‚ - Ø«ÙˆØ§Ù†ÙŠ)\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, price_type, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting tick download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    \n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=15)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    successful_downloads += 1\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    try:\n",
    "                        data = lzma.decompress(r.content)\n",
    "                        for i in range(0, len(data), 20):\n",
    "                            chunk = data[i:i+20]\n",
    "                            if len(chunk) < 20: \n",
    "                                continue\n",
    "                            # ÙÙƒ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ time_offset, ask, bid, ask_volume, bid_volume\n",
    "                            t_off, ask, bid, ask_vol, bid_vol = struct.unpack(\">IIfff\", chunk)\n",
    "                            \n",
    "                            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØµØ­ÙŠØ­\n",
    "                            tick_time = day + timedelta(hours=hour, milliseconds=t_off)\n",
    "                            tick_time += timedelta(hours=gmt_offset)\n",
    "                            \n",
    "                            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ù…Ø¹Ø§Ù‹\n",
    "                            formatted_time = tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                            \n",
    "                            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                                row = [formatted_time, round(bid, 5), round(bid_vol, 2)]\n",
    "                            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                                row = [formatted_time, round(ask, 5), round(ask_vol, 2)]\n",
    "                            else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§ - Ø¯Ù…Ø¬ Bid Ùˆ Ask\n",
    "                                row = [formatted_time, round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                            \n",
    "                            all_ticks.append(row)\n",
    "                    except lzma.LZMAError as e:\n",
    "                        print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                        continue\n",
    "                elif r.status_code == 404:\n",
    "                    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®/Ø§Ù„Ø³Ø§Ø¹Ø© (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')} {hour}h\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                continue\n",
    "                \n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total hours processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total ticks collected: {len(all_ticks)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
    "    base_filename = f\"{symbol}_tick_merged.csv\"\n",
    "    filename = get_safe_filename(path, base_filename)\n",
    "    \n",
    "    # ØªØ­Ø¯ÙŠØ¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "        headers = [\"datetime\", \"bid\", \"bid_volume\"]\n",
    "    elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "        headers = [\"datetime\", \"ask\", \"ask_volume\"]\n",
    "    else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§\n",
    "        headers = [\"datetime\", \"bid\", \"ask\", \"bid_volume\", \"ask_volume\"]\n",
    "    \n",
    "    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©\n",
    "    final_filename, success, message = safe_file_write(filename, headers, all_ticks if all_ticks else None)\n",
    "    \n",
    "    if success:\n",
    "        file_size = os.path.getsize(final_filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(final_filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        if message:\n",
    "            print(f\"â„¹ï¸ Note: {message}\")\n",
    "        if not all_ticks:\n",
    "            print(\"âš ï¸ No tick data was collected. File contains headers only.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ {message}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, start, end, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting candle download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    print(f\"ğŸ“Š Timeframe code: {tf_code}\")\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                successful_downloads += 1\n",
    "                downloaded_bytes += len(r.content)\n",
    "                try:\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    for i in range(0, len(data), 24):  # Changed from 20 to 24 bytes for candles\n",
    "                        chunk = data[i:i+24]\n",
    "                        if len(chunk) < 24: \n",
    "                            continue\n",
    "                        # ÙÙƒ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø´Ù…ÙˆØ¹\n",
    "                        utc_offset, open_, high, low, close, vol = struct.unpack(\">IIffff\", chunk[:24])\n",
    "                        \n",
    "                        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø´Ù…Ø¹Ø©\n",
    "                        candle_time = day + timedelta(seconds=utc_offset) + timedelta(hours=gmt_offset)\n",
    "                        \n",
    "                        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ù…Ø¹Ø§Ù‹\n",
    "                        formatted_time = candle_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        \n",
    "                        row = [formatted_time, round(open_, 5), round(high, 5),\n",
    "                               round(low, 5), round(close, 5), round(vol, 2)]\n",
    "                        all_data.append(row)\n",
    "                except lzma.LZMAError as e:\n",
    "                    print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "                    continue\n",
    "            elif r.status_code == 404:\n",
    "                # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total days processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total candles collected: {len(all_data)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù„Ù„Ø´Ù…ÙˆØ¹\n",
    "    base_filename = f\"{symbol}_candles_{tf_code}_merged.csv\"\n",
    "    filename = get_safe_filename(path, base_filename)\n",
    "    headers = [\"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    \n",
    "    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©\n",
    "    final_filename, success, message = safe_file_write(filename, headers, all_data if all_data else None)\n",
    "    \n",
    "    if success:\n",
    "        file_size = os.path.getsize(final_filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(final_filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        if message:\n",
    "            print(f\"â„¹ï¸ Note: {message}\")\n",
    "        if not all_data:\n",
    "            print(\"âš ï¸ No candle data was collected. File contains headers only.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ {message}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    try:\n",
    "        data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "        category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "        symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "        gmt_offset = get_gmt_offset()\n",
    "        start = get_date_input(\"Start date\")\n",
    "        end = get_date_input(\"End date\")\n",
    "\n",
    "        if start > end:\n",
    "            print(\"âŒ End date must be after start date.\")\n",
    "            exit()\n",
    "\n",
    "        print(f\"\\nğŸ“ Configuration Summary:\")\n",
    "        print(f\"   - Symbol: {symbol}\")\n",
    "        print(f\"   - Data type: {data_type}\")\n",
    "        print(f\"   - Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   - GMT offset: {gmt_offset}\")\n",
    "        print(f\"   - Save path: {DEFAULT_SAVE_PATH}\")\n",
    "\n",
    "        success = False\n",
    "        if data_type == \"Tick\":\n",
    "            price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "            print(f\"   - Price type: {price_type}\")\n",
    "            success = download_tick(symbol, start, end, price_type, gmt_offset)\n",
    "        else:\n",
    "            tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "            tf_code = CANDLE_FRAME_CODES[tf]\n",
    "            print(f\"   - Timeframe: {tf}\")\n",
    "            success = download_candles(symbol, tf_code, start, end, gmt_offset)\n",
    "\n",
    "        if success:\n",
    "            print(\"\\nâœ… Download completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Download completed with errors!\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâ¹ï¸ Download interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd2fb1a6-796e-47d5-b9b8-1229047846bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool\n",
      "========================================\n",
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2020-01-01\n",
      "End date (e.g. 2020-01-01):  2020-01-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Configuration Summary:\n",
      "   - Symbol: XAUUSD\n",
      "   - Data type: Candlestick\n",
      "   - Date range: 2020-01-01 to 2020-01-02\n",
      "   - GMT offset: 0\n",
      "   - Save path: C:\\Users\\Access\\Downloads\n",
      "\n",
      "ğŸ”¹ Select timeframe\n",
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Timeframe: 15m\n",
      "\n",
      "ğŸ”„ Starting candle download for XAUUSD...\n",
      "ğŸ“… Date range: 2020-01-01 to 2020-01-02\n",
      "â° GMT offset: 0\n",
      "ğŸ“Š Timeframe code: M15\n",
      "[==================================================] 100.00% | 0.0MB of 0.0MB | ETA: 0h 0m 0s\n",
      "\n",
      "ğŸ“Š Download Summary:\n",
      "   - Total days processed: 2\n",
      "   - Successful downloads: 0\n",
      "   - Total candles collected: 0\n",
      "âœ… File saved: C:\\Users\\Access\\Downloads\\XAUUSD_candles_M15.csv\n",
      "ğŸ“¦ Size: 0.00 MB\n",
      "âš ï¸ No candle data was collected. File contains headers only.\n",
      "\n",
      "âœ… Download completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "def get_save_path():\n",
    "    \"\"\"ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ø¢Ù…Ù†\"\"\"\n",
    "    possible_paths = [\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Downloads\"),  # Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Documents\"),  # Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
    "        os.path.expanduser(\"~\"),  # Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…\n",
    "        os.getcwd()  # Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            # Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙƒØªØ§Ø¨Ø©\n",
    "            test_file = os.path.join(path, \"test_write.tmp\")\n",
    "            with open(test_file, \"w\") as f:\n",
    "                f.write(\"test\")\n",
    "            os.remove(test_file)\n",
    "            return path\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return os.getcwd()  # Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙƒØ®ÙŠØ§Ø± Ø£Ø®ÙŠØ±\n",
    "\n",
    "DEFAULT_SAVE_PATH = get_save_path()\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"M1\", \"5m\": \"M5\", \"15m\": \"M15\", \"1h\": \"H1\", \"4h\": \"H4\", \"1d\": \"D1\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_safe_filename(base_path, filename):\n",
    "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù† Ù…Ø¹ ØªØ¬Ù†Ø¨ ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡\"\"\"\n",
    "    full_path = os.path.join(base_path, filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        return full_path\n",
    "    \n",
    "    # ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø¶Ø§ÙØ© Ø±Ù‚Ù… Ù…ØªØ³Ù„Ø³Ù„\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(base_path, f\"{name}_{counter}{ext}\")):\n",
    "        counter += 1\n",
    "    return os.path.join(base_path, f\"{name}_{counter}{ext}\")\n",
    "\n",
    "def safe_file_write(filename, headers, data, encoding='utf-8'):\n",
    "    \"\"\"ÙƒØªØ§Ø¨Ø© Ø¢Ù…Ù†Ø© Ù„Ù„Ù…Ù„Ù Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª\"\"\"\n",
    "    try:\n",
    "        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯\n",
    "        with open(filename, \"w\", newline='', encoding=encoding) as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(headers)\n",
    "            if data:\n",
    "                writer.writerows(data)\n",
    "        return filename, True, None\n",
    "        \n",
    "    except PermissionError:\n",
    "        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª\n",
    "        downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "        try:\n",
    "            os.makedirs(downloads_path, exist_ok=True)\n",
    "            alt_filename = os.path.join(downloads_path, os.path.basename(filename))\n",
    "            alt_filename = get_safe_filename(downloads_path, os.path.basename(filename))\n",
    "            \n",
    "            with open(alt_filename, \"w\", newline='', encoding=encoding) as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(headers)\n",
    "                if data:\n",
    "                    writer.writerows(data)\n",
    "            return alt_filename, True, \"Saved to Downloads folder due to permission issue\"\n",
    "            \n",
    "        except Exception as e2:\n",
    "            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\n",
    "            try:\n",
    "                desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "                os.makedirs(desktop_path, exist_ok=True)\n",
    "                desktop_filename = os.path.join(desktop_path, os.path.basename(filename))\n",
    "                desktop_filename = get_safe_filename(desktop_path, os.path.basename(filename))\n",
    "                \n",
    "                with open(desktop_filename, \"w\", newline='', encoding=encoding) as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(headers)\n",
    "                    if data:\n",
    "                        writer.writerows(data)\n",
    "                return desktop_filename, True, \"Saved to Desktop due to permission issue\"\n",
    "                \n",
    "            except Exception as e3:\n",
    "                return filename, False, f\"Failed to save file: {str(e3)}\"\n",
    "                \n",
    "def format_seconds(seconds):\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØµÙŠØºØ© (Ø³Ø§Ø¹Ø§Øª - Ø¯Ù‚Ø§Ø¦Ù‚ - Ø«ÙˆØ§Ù†ÙŠ)\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, price_type, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting tick download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    \n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour:02d}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=15)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    successful_downloads += 1\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    try:\n",
    "                        data = lzma.decompress(r.content)\n",
    "                        # ÙƒÙ„ tick ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 20 Ø¨Ø§ÙŠØª: 4 Ø¨Ø§ÙŠØª Ù„Ù„ÙˆÙ‚Øª + 4*4 Ø¨Ø§ÙŠØª Ù„Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„Ø­Ø¬Ù…\n",
    "                        for i in range(0, len(data), 20):\n",
    "                            chunk = data[i:i+20]\n",
    "                            if len(chunk) < 20: \n",
    "                                continue\n",
    "                            \n",
    "                            # ÙÙƒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: time_delta(ms), ask, bid, ask_volume, bid_volume\n",
    "                            try:\n",
    "                                time_delta, ask, bid, ask_vol, bid_vol = struct.unpack(\">Iffff\", chunk)\n",
    "                                \n",
    "                                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©\n",
    "                                # Dukascopy ÙŠØ®Ø²Ù† Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ù…Ù‚Ø³ÙˆÙ…Ø© Ø¹Ù„Ù‰ 100000\n",
    "                                if symbol in [\"USDJPY\", \"EURJPY\", \"GBPJPY\", \"AUDJPY\", \"NZDJPY\", \"CADJPY\", \"CHFJPY\"]:\n",
    "                                    # Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„ÙŠÙ† Ù„Ù‡Ø§ 3 Ø®Ø§Ù†Ø§Øª Ø¹Ø´Ø±ÙŠØ©\n",
    "                                    ask = ask / 1000\n",
    "                                    bid = bid / 1000\n",
    "                                else:\n",
    "                                    # Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ù„Ù‡Ø§ 5 Ø®Ø§Ù†Ø§Øª Ø¹Ø´Ø±ÙŠØ©\n",
    "                                    ask = ask / 100000\n",
    "                                    bid = bid / 100000\n",
    "                                \n",
    "                                # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØµØ­ÙŠØ­\n",
    "                                tick_time = day.replace(hour=hour) + timedelta(milliseconds=time_delta)\n",
    "                                tick_time += timedelta(hours=gmt_offset)\n",
    "                                \n",
    "                                # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ù…Ø¹Ø§Ù‹\n",
    "                                formatted_time = tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                                \n",
    "                                if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                                    row = [formatted_time, round(bid, 5), round(bid_vol, 2)]\n",
    "                                elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                                    row = [formatted_time, round(ask, 5), round(ask_vol, 2)]\n",
    "                                else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§ - Ø¯Ù…Ø¬ Bid Ùˆ Ask\n",
    "                                    row = [formatted_time, round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                                \n",
    "                                all_ticks.append(row)\n",
    "                            except struct.error as se:\n",
    "                                print(f\"\\nâš ï¸ Struct error: {se} for chunk length {len(chunk)}\")\n",
    "                                continue\n",
    "                                \n",
    "                    except lzma.LZMAError as e:\n",
    "                        print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')} {hour:02d}h: {e}\")\n",
    "                        continue\n",
    "                elif r.status_code == 404:\n",
    "                    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®/Ø§Ù„Ø³Ø§Ø¹Ø© (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')} {hour:02d}h\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')} {hour:02d}h: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')} {hour:02d}h: {e}\")\n",
    "                continue\n",
    "                \n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total hours processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total ticks collected: {len(all_ticks)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
    "    base_filename = f\"{symbol}_tick_{price_type.replace(' ', '_').replace('ÙÙ‚Ø·', 'only').replace('ÙƒÙ„Ø§Ù‡Ù…Ø§', 'both')}.csv\"\n",
    "    filename = get_safe_filename(path, base_filename)\n",
    "    \n",
    "    # ØªØ­Ø¯ÙŠØ¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "        headers = [\"datetime\", \"bid\", \"bid_volume\"]\n",
    "    elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "        headers = [\"datetime\", \"ask\", \"ask_volume\"]\n",
    "    else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§\n",
    "        headers = [\"datetime\", \"bid\", \"ask\", \"bid_volume\", \"ask_volume\"]\n",
    "    \n",
    "    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©\n",
    "    final_filename, success, message = safe_file_write(filename, headers, all_ticks if all_ticks else None)\n",
    "    \n",
    "    if success:\n",
    "        file_size = os.path.getsize(final_filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(final_filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        if message:\n",
    "            print(f\"â„¹ï¸ Note: {message}\")\n",
    "        if not all_ticks:\n",
    "            print(\"âš ï¸ No tick data was collected. File contains headers only.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ {message}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, start, end, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting candle download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    print(f\"ğŸ“Š Timeframe code: {tf_code}\")\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                successful_downloads += 1\n",
    "                downloaded_bytes += len(r.content)\n",
    "                try:\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    # ÙƒÙ„ Ø´Ù…Ø¹Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 24 Ø¨Ø§ÙŠØª: 4 Ù„Ù„ÙˆÙ‚Øª + 5*4 Ù„Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„Ø­Ø¬Ù…\n",
    "                    for i in range(0, len(data), 24):\n",
    "                        chunk = data[i:i+24]\n",
    "                        if len(chunk) < 24: \n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            # ÙÙƒ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø´Ù…ÙˆØ¹: time, open, high, low, close, volume\n",
    "                            time_offset, open_price, high_price, low_price, close_price, volume = struct.unpack(\">Ifffff\", chunk)\n",
    "                            \n",
    "                            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©\n",
    "                            if symbol in [\"USDJPY\", \"EURJPY\", \"GBPJPY\", \"AUDJPY\", \"NZDJPY\", \"CADJPY\", \"CHFJPY\"]:\n",
    "                                # Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„ÙŠÙ† Ù„Ù‡Ø§ 3 Ø®Ø§Ù†Ø§Øª Ø¹Ø´Ø±ÙŠØ©\n",
    "                                open_price = open_price / 1000\n",
    "                                high_price = high_price / 1000\n",
    "                                low_price = low_price / 1000\n",
    "                                close_price = close_price / 1000\n",
    "                            else:\n",
    "                                # Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ù„Ù‡Ø§ 5 Ø®Ø§Ù†Ø§Øª Ø¹Ø´Ø±ÙŠØ©\n",
    "                                open_price = open_price / 100000\n",
    "                                high_price = high_price / 100000\n",
    "                                low_price = low_price / 100000\n",
    "                                close_price = close_price / 100000\n",
    "                            \n",
    "                            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø´Ù…Ø¹Ø©\n",
    "                            candle_time = day + timedelta(seconds=time_offset) + timedelta(hours=gmt_offset)\n",
    "                            \n",
    "                            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ù…Ø¹Ø§Ù‹\n",
    "                            formatted_time = candle_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                            \n",
    "                            row = [formatted_time, round(open_price, 5), round(high_price, 5),\n",
    "                                   round(low_price, 5), round(close_price, 5), round(volume, 2)]\n",
    "                            all_data.append(row)\n",
    "                        except struct.error as se:\n",
    "                            print(f\"\\nâš ï¸ Struct error: {se} for chunk length {len(chunk)}\")\n",
    "                            continue\n",
    "                            \n",
    "                except lzma.LZMAError as e:\n",
    "                    print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "                    continue\n",
    "            elif r.status_code == 404:\n",
    "                # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total days processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total candles collected: {len(all_data)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù„Ù„Ø´Ù…ÙˆØ¹\n",
    "    base_filename = f\"{symbol}_candles_{tf_code}.csv\"\n",
    "    filename = get_safe_filename(path, base_filename)\n",
    "    headers = [\"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    \n",
    "    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©\n",
    "    final_filename, success, message = safe_file_write(filename, headers, all_data if all_data else None)\n",
    "    \n",
    "    if success:\n",
    "        file_size = os.path.getsize(final_filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(final_filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        if message:\n",
    "            print(f\"â„¹ï¸ Note: {message}\")\n",
    "        if not all_data:\n",
    "            print(\"âš ï¸ No candle data was collected. File contains headers only.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ {message}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    try:\n",
    "        data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "        category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "        symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "        gmt_offset = get_gmt_offset()\n",
    "        start = get_date_input(\"Start date\")\n",
    "        end = get_date_input(\"End date\")\n",
    "\n",
    "        if start > end:\n",
    "            print(\"âŒ End date must be after start date.\")\n",
    "            exit()\n",
    "\n",
    "        print(f\"\\nğŸ“ Configuration Summary:\")\n",
    "        print(f\"   - Symbol: {symbol}\")\n",
    "        print(f\"   - Data type: {data_type}\")\n",
    "        print(f\"   - Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   - GMT offset: {gmt_offset}\")\n",
    "        print(f\"   - Save path: {DEFAULT_SAVE_PATH}\")\n",
    "\n",
    "        success = False\n",
    "        if data_type == \"Tick\":\n",
    "            price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "            print(f\"   - Price type: {price_type}\")\n",
    "            success = download_tick(symbol, start, end, price_type, gmt_offset)\n",
    "        else:\n",
    "            tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "            tf_code = CANDLE_FRAME_CODES[tf]\n",
    "            print(f\"   - Timeframe: {tf}\")\n",
    "            success = download_candles(symbol, tf_code, start, end, gmt_offset)\n",
    "\n",
    "        if success:\n",
    "            print(\"\\nâœ… Download completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Download completed with errors!\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâ¹ï¸ Download interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f47087d-1669-40fb-b9cd-02428ccc5a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool\n",
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2020-01-01\n",
      "End date (e.g. 2020-01-01):  2020-01-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select timeframe\n",
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.00% | 0.0MB of 0.0MB | ETA: 0h 0m 0s\n",
      "âœ… Download completed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "# ÙŠØªÙ… Ø§Ù„Ø­ÙØ¸ Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ Jupyter Ø§Ù„Ø­Ø§Ù„ÙŠ\n",
    "DEFAULT_SAVE_PATH = os.getcwd()\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"60\", \"5m\": \"300\", \"15m\": \"900\", \"1h\": \"3600\", \"4h\": \"14400\", \"1d\": \"86400\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def format_seconds(seconds):\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, price_type, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=10)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    for i in range(0, len(data), 20):\n",
    "                        chunk = data[i:i+20]\n",
    "                        if len(chunk) < 20: continue\n",
    "                        t_off, ask, bid, ask_vol, bid_vol = struct.unpack(\" >IIfff\", chunk)\n",
    "                        tick_time = day + timedelta(hours=hour, milliseconds=t_off)\n",
    "                        tick_time += timedelta(hours=gmt_offset)\n",
    "                        row = [tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]]\n",
    "                        if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                            row += [round(bid, 5), round(bid_vol, 2)]\n",
    "                        elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                            row += [round(ask, 5), round(ask_vol, 2)]\n",
    "                        else:\n",
    "                            row += [round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                        all_ticks.append(row)\n",
    "            except:\n",
    "                pass\n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    if all_ticks:\n",
    "        filename = os.path.join(path, f\"{symbol}_tick_merged.csv\")\n",
    "        with open(filename, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"bid\", \"bid_vol\"])\n",
    "            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"ask\", \"ask_vol\"])\n",
    "            else:\n",
    "                writer.writerow([\"time\", \"bid\", \"ask\", \"bid_vol\", \"ask_vol\"])\n",
    "            all_ticks.sort(key=lambda x: x[0])\n",
    "            writer.writerows(all_ticks)\n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ù…ÙˆØ¹ ==========\n",
    "def download_candles(symbol, tf_code, start, end, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                downloaded_bytes += len(r.content)\n",
    "                data = lzma.decompress(r.content)\n",
    "                for i in range(0, len(data), 20):\n",
    "                    chunk = data[i:i+20]\n",
    "                    if len(chunk) < 20: continue\n",
    "                    utc_offset, open_, high, low, close, vol = struct.unpack(\">IIffff\", chunk[:20])\n",
    "                    candle_time = day + timedelta(seconds=utc_offset) + timedelta(hours=gmt_offset)\n",
    "                    row = [candle_time.strftime(\"%Y-%m-%d %H:%M:%S\"), round(open_, 5), round(high, 5),\n",
    "                           round(low, 5), round(close, 5), round(vol, 2)]\n",
    "                    all_data.append(row)\n",
    "        except:\n",
    "            pass\n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    if all_data:\n",
    "        filename = os.path.join(path, f\"{symbol}_candles_{tf_code}_merged.csv\")\n",
    "        with open(filename, \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "            all_data.sort(key=lambda x: x[0])\n",
    "            writer.writerows(all_data)\n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool\")\n",
    "\n",
    "    data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "    category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "    symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "    gmt_offset = get_gmt_offset()\n",
    "    start = get_date_input(\"Start date\")\n",
    "    end = get_date_input(\"End date\")\n",
    "\n",
    "    if start > end:\n",
    "        print(\"âŒ End date must be after start date.\")\n",
    "        exit()\n",
    "\n",
    "    if data_type == \"Tick\":\n",
    "        price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "        download_tick(symbol, start, end, price_type, gmt_offset)\n",
    "    else:\n",
    "        tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "        tf_code = CANDLE_FRAME_CODES[tf]\n",
    "        download_candles(symbol, tf_code, start, end, gmt_offset)\n",
    "\n",
    "    print(\"\\nâœ… Download completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45e85a96-db5b-4135-926d-6fb9a7e067d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool - Enhanced Version\n",
      "==================================================\n",
      "\n",
      "ğŸ”¹ Select action\n",
      "1. Download New Data\n",
      "2. View Previous Downloads\n",
      "3. Merge Files\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2020-01-01\n",
      "End date (e.g. 2020-01-01):  2020-01-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Configuration Summary:\n",
      "   - Symbol: XAUUSD\n",
      "   - Data type: Candlestick\n",
      "   - Date range: 2020-01-01 to 2020-01-02\n",
      "   - GMT offset: 0\n",
      "   - Save path: C:\\Users\\Access\\Downloads\n",
      "\n",
      "ğŸ”¹ Select timeframe\n",
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Timeframe: 15m\n",
      "   - Columns: datetime, open_price, high_price, low_price, close_price, volume\n",
      "\n",
      "ğŸ”„ Starting candle download for XAUUSD...\n",
      "ğŸ“… Date range: 2020-01-01 to 2020-01-02\n",
      "â° GMT offset: 0\n",
      "ğŸ“Š Timeframe code: M15\n",
      "ğŸ“Š Column structure: datetime, open_price, high_price, low_price, close_price, volume\n",
      "[==================================================] 100.00% | 0.0MB of 0.0MB | ETA: 0h 0m 0s\n",
      "\n",
      "ğŸ“Š Download Summary:\n",
      "   - Total days processed: 2\n",
      "   - Successful downloads: 0\n",
      "   - Total candles collected: 0\n",
      "âœ… File saved: C:\\Users\\Access\\Downloads\\XAUUSD_candles_15m_20200101_20200102.csv\n",
      "ğŸ“¦ Size: 0.00 MB\n",
      "âš ï¸ No candle data was collected. File contains headers only.\n",
      "ğŸ“‹ Metadata saved to: C:\\Users\\Access\\Downloads\\downloads_metadata.json\n",
      "\n",
      "âœ… Download completed successfully!\n",
      "\n",
      "ğŸ’¡ Tip: You can now use 'Merge Files' option to combine multiple downloads.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "def get_save_path():\n",
    "    \"\"\"ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ø¢Ù…Ù†\"\"\"\n",
    "    possible_paths = [\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Downloads\"),  # Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Documents\"),  # Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\n",
    "        os.path.expanduser(\"~\"),  # Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…\n",
    "        os.getcwd()  # Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            # Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙƒØªØ§Ø¨Ø©\n",
    "            test_file = os.path.join(path, \"test_write.tmp\")\n",
    "            with open(test_file, \"w\") as f:\n",
    "                f.write(\"test\")\n",
    "            os.remove(test_file)\n",
    "            return path\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return os.getcwd()  # Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙƒØ®ÙŠØ§Ø± Ø£Ø®ÙŠØ±\n",
    "\n",
    "DEFAULT_SAVE_PATH = get_save_path()\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"M1\", \"5m\": \"M5\", \"15m\": \"M15\", \"1h\": \"H1\", \"4h\": \"H4\", \"1d\": \"D1\"\n",
    "}\n",
    "\n",
    "# ========== ØªØ¹Ø±ÙŠÙ Ø«Ø§Ø¨Øª Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ==========\n",
    "COLUMN_SCHEMAS = {\n",
    "    \"tick_bid\": [\"datetime\", \"bid_price\", \"bid_volume\"],\n",
    "    \"tick_ask\": [\"datetime\", \"ask_price\", \"ask_volume\"], \n",
    "    \"tick_both\": [\"datetime\", \"bid_price\", \"ask_price\", \"bid_volume\", \"ask_volume\"],\n",
    "    \"candles\": [\"datetime\", \"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\"]\n",
    "}\n",
    "\n",
    "# ========== Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø© ==========\n",
    "def save_download_metadata(symbol, data_type, start_date, end_date, filename, columns, price_type=None, timeframe=None):\n",
    "    \"\"\"Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙŠ Ù…Ù„Ù Ù…Ù†ÙØµÙ„ Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\"\"\"\n",
    "    metadata_file = os.path.join(DEFAULT_SAVE_PATH, \"downloads_metadata.json\")\n",
    "    \n",
    "    metadata = {\n",
    "        \"symbol\": symbol,\n",
    "        \"data_type\": data_type,\n",
    "        \"start_date\": start_date.isoformat(),\n",
    "        \"end_date\": end_date.isoformat(),\n",
    "        \"filename\": os.path.basename(filename),\n",
    "        \"full_path\": os.path.abspath(filename),\n",
    "        \"columns\": columns,\n",
    "        \"download_timestamp\": datetime.now().isoformat(),\n",
    "        \"price_type\": price_type,\n",
    "        \"timeframe\": timeframe\n",
    "    }\n",
    "    \n",
    "    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©\n",
    "    existing_data = []\n",
    "    if os.path.exists(metadata_file):\n",
    "        try:\n",
    "            with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        except:\n",
    "            existing_data = []\n",
    "    \n",
    "    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©\n",
    "    existing_data.append(metadata)\n",
    "    \n",
    "    # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø«Ø©\n",
    "    try:\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"ğŸ“‹ Metadata saved to: {metadata_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Warning: Could not save metadata: {e}\")\n",
    "\n",
    "def load_download_metadata():\n",
    "    \"\"\"Ù‚Ø±Ø§Ø¡Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©\"\"\"\n",
    "    metadata_file = os.path.join(DEFAULT_SAVE_PATH, \"downloads_metadata.json\")\n",
    "    if os.path.exists(metadata_file):\n",
    "        try:\n",
    "            with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def show_previous_downloads():\n",
    "    \"\"\"Ø¹Ø±Ø¶ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©\"\"\"\n",
    "    metadata = load_download_metadata()\n",
    "    if not metadata:\n",
    "        print(\"ğŸ“‚ No previous downloads found.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nğŸ“‚ Previous Downloads:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, item in enumerate(metadata, 1):\n",
    "        print(f\"{i}. {item['symbol']} - {item['data_type']}\")\n",
    "        print(f\"   ğŸ“… Date: {item['start_date']} to {item['end_date']}\")\n",
    "        print(f\"   ğŸ“„ File: {item['filename']}\")\n",
    "        print(f\"   ğŸ“Š Columns: {', '.join(item['columns'])}\")\n",
    "        if item.get('price_type'):\n",
    "            print(f\"   ğŸ’° Price Type: {item['price_type']}\")\n",
    "        if item.get('timeframe'):\n",
    "            print(f\"   â±ï¸ Timeframe: {item['timeframe']}\")\n",
    "        print(f\"   ğŸ“ Path: {item['full_path']}\")\n",
    "        print()\n",
    "\n",
    "def get_standardized_columns(data_type, price_type=None):\n",
    "    \"\"\"Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©\"\"\"\n",
    "    if data_type == \"Tick\":\n",
    "        if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "            return COLUMN_SCHEMAS[\"tick_bid\"]\n",
    "        elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "            return COLUMN_SCHEMAS[\"tick_ask\"]\n",
    "        else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§\n",
    "            return COLUMN_SCHEMAS[\"tick_both\"]\n",
    "    else:  # Candlestick\n",
    "        return COLUMN_SCHEMAS[\"candles\"]\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_safe_filename(base_path, filename):\n",
    "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù† Ù…Ø¹ ØªØ¬Ù†Ø¨ ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡\"\"\"\n",
    "    full_path = os.path.join(base_path, filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        return full_path\n",
    "    \n",
    "    # ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø¶Ø§ÙØ© Ø±Ù‚Ù… Ù…ØªØ³Ù„Ø³Ù„\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(base_path, f\"{name}_{counter}{ext}\")):\n",
    "        counter += 1\n",
    "    return os.path.join(base_path, f\"{name}_{counter}{ext}\")\n",
    "\n",
    "def safe_file_write(filename, headers, data, encoding='utf-8'):\n",
    "    \"\"\"ÙƒØªØ§Ø¨Ø© Ø¢Ù…Ù†Ø© Ù„Ù„Ù…Ù„Ù Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª\"\"\"\n",
    "    try:\n",
    "        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯\n",
    "        with open(filename, \"w\", newline='', encoding=encoding) as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(headers)\n",
    "            if data:\n",
    "                writer.writerows(data)\n",
    "        return filename, True, None\n",
    "        \n",
    "    except PermissionError:\n",
    "        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„Ø§Øª\n",
    "        downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "        try:\n",
    "            os.makedirs(downloads_path, exist_ok=True)\n",
    "            alt_filename = os.path.join(downloads_path, os.path.basename(filename))\n",
    "            alt_filename = get_safe_filename(downloads_path, os.path.basename(filename))\n",
    "            \n",
    "            with open(alt_filename, \"w\", newline='', encoding=encoding) as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(headers)\n",
    "                if data:\n",
    "                    writer.writerows(data)\n",
    "            return alt_filename, True, \"Saved to Downloads folder due to permission issue\"\n",
    "            \n",
    "        except Exception as e2:\n",
    "            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\n",
    "            try:\n",
    "                desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "                os.makedirs(desktop_path, exist_ok=True)\n",
    "                desktop_filename = os.path.join(desktop_path, os.path.basename(filename))\n",
    "                desktop_filename = get_safe_filename(desktop_path, os.path.basename(filename))\n",
    "                \n",
    "                with open(desktop_filename, \"w\", newline='', encoding=encoding) as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(headers)\n",
    "                    if data:\n",
    "                        writer.writerows(data)\n",
    "                return desktop_filename, True, \"Saved to Desktop due to permission issue\"\n",
    "                \n",
    "            except Exception as e3:\n",
    "                return filename, False, f\"Failed to save file: {str(e3)}\"\n",
    "                \n",
    "def format_seconds(seconds):\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØµÙŠØºØ© (Ø³Ø§Ø¹Ø§Øª - Ø¯Ù‚Ø§Ø¦Ù‚ - Ø«ÙˆØ§Ù†ÙŠ)\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, price_type, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting tick download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    \n",
    "    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©\n",
    "    headers = get_standardized_columns(\"Tick\", price_type)\n",
    "    print(f\"ğŸ“Š Column structure: {', '.join(headers)}\")\n",
    "    \n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour:02d}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=15)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    successful_downloads += 1\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    try:\n",
    "                        data = lzma.decompress(r.content)\n",
    "                        # ÙƒÙ„ tick ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 20 Ø¨Ø§ÙŠØª: 4 Ø¨Ø§ÙŠØª Ù„Ù„ÙˆÙ‚Øª + 4*4 Ø¨Ø§ÙŠØª Ù„Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„Ø­Ø¬Ù…\n",
    "                        for i in range(0, len(data), 20):\n",
    "                            chunk = data[i:i+20]\n",
    "                            if len(chunk) < 20: \n",
    "                                continue\n",
    "                            \n",
    "                            # ÙÙƒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: time_delta(ms), ask, bid, ask_volume, bid_volume\n",
    "                            try:\n",
    "                                time_delta, ask, bid, ask_vol, bid_vol = struct.unpack(\">Iffff\", chunk)\n",
    "                                \n",
    "                                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©\n",
    "                                # Dukascopy ÙŠØ®Ø²Ù† Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ù…Ù‚Ø³ÙˆÙ…Ø© Ø¹Ù„Ù‰ 100000\n",
    "                                if symbol in [\"USDJPY\", \"EURJPY\", \"GBPJPY\", \"AUDJPY\", \"NZDJPY\", \"CADJPY\", \"CHFJPY\"]:\n",
    "                                    # Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„ÙŠÙ† Ù„Ù‡Ø§ 3 Ø®Ø§Ù†Ø§Øª Ø¹Ø´Ø±ÙŠØ©\n",
    "                                    ask = ask / 1000\n",
    "                                    bid = bid / 1000\n",
    "                                else:\n",
    "                                    # Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ù„Ù‡Ø§ 5 Ø®Ø§Ù†Ø§Øª Ø¹Ø´Ø±ÙŠØ©\n",
    "                                    ask = ask / 100000\n",
    "                                    bid = bid / 100000\n",
    "                                \n",
    "                                # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØµØ­ÙŠØ­\n",
    "                                tick_time = day.replace(hour=hour) + timedelta(milliseconds=time_delta)\n",
    "                                tick_time += timedelta(hours=gmt_offset)\n",
    "                                \n",
    "                                # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ù…Ø¹Ø§Ù‹\n",
    "                                formatted_time = tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                                \n",
    "                                # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©\n",
    "                                if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                                    row = [formatted_time, round(bid, 5), round(bid_vol, 2)]\n",
    "                                elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                                    row = [formatted_time, round(ask, 5), round(ask_vol, 2)]\n",
    "                                else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§ - Ø¯Ù…Ø¬ Bid Ùˆ Ask\n",
    "                                    row = [formatted_time, round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                                \n",
    "                                all_ticks.append(row)\n",
    "                            except struct.error as se:\n",
    "                                print(f\"\\nâš ï¸ Struct error: {se} for chunk length {len(chunk)}\")\n",
    "                                continue\n",
    "                                \n",
    "                    except lzma.LZMAError as e:\n",
    "                        print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')} {hour:02d}h: {e}\")\n",
    "                        continue\n",
    "                elif r.status_code == 404:\n",
    "                    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®/Ø§Ù„Ø³Ø§Ø¹Ø© (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')} {hour:02d}h\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')} {hour:02d}h: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')} {hour:02d}h: {e}\")\n",
    "                continue\n",
    "                \n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total hours processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total ticks collected: {len(all_ticks)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ø³Ù… Ù…ÙˆØ­Ø¯\n",
    "    price_type_code = {\n",
    "        \"Bid ÙÙ‚Ø·\": \"bid\",\n",
    "        \"Ask ÙÙ‚Ø·\": \"ask\", \n",
    "        \"ÙƒÙ„Ø§Ù‡Ù…Ø§\": \"both\"\n",
    "    }[price_type]\n",
    "    \n",
    "    base_filename = f\"{symbol}_tick_{price_type_code}_{start.strftime('%Y%m%d')}_{end.strftime('%Y%m%d')}.csv\"\n",
    "    filename = get_safe_filename(path, base_filename)\n",
    "    \n",
    "    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©\n",
    "    final_filename, success, message = safe_file_write(filename, headers, all_ticks if all_ticks else None)\n",
    "    \n",
    "    if success:\n",
    "        file_size = os.path.getsize(final_filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(final_filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        if message:\n",
    "            print(f\"â„¹ï¸ Note: {message}\")\n",
    "        if not all_ticks:\n",
    "            print(\"âš ï¸ No tick data was collected. File contains headers only.\")\n",
    "        \n",
    "        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„\n",
    "        save_download_metadata(symbol, \"Tick\", start, end, final_filename, headers, price_type)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ {message}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, timeframe, start, end, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting candle download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    print(f\"ğŸ“Š Timeframe code: {tf_code}\")\n",
    "\n",
    "    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©\n",
    "    headers = get_standardized_columns(\"Candlestick\")\n",
    "    print(f\"ğŸ“Š Column structure: {', '.join(headers)}\")\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                successful_downloads += 1\n",
    "                downloaded_bytes += len(r.content)\n",
    "                try:\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    # ÙƒÙ„ Ø´Ù…Ø¹Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 24 Ø¨Ø§ÙŠØª: 4 Ù„Ù„ÙˆÙ‚Øª + 5*4 Ù„Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„Ø­Ø¬Ù…\n",
    "                    for i in range(0, len(data), 24):\n",
    "                        chunk = data[i:i+24]\n",
    "                        if len(chunk) < 24: \n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            # ÙÙƒ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø´Ù…ÙˆØ¹: time, open, high, low, close, volume\n",
    "                            time_offset, open_price, high_price, low_price, close_price, volume = struct.unpack(\">Ifffff\", chunk)\n",
    "                            \n",
    "                            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©\n",
    "                            if symbol in [\"USDJPY\", \"EURJPY\", \"GBPJPY\", \"AUDJPY\", \"NZDJPY\", \"CADJPY\", \"CHFJPY\"]:\n",
    "                                # Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„ÙŠÙ† Ù„Ù‡Ø§ 3 Ø®Ø§Ù†Ø§Øª Ø¹Ø´Ø±ÙŠØ©\n",
    "                                open_price = open_price / 1000\n",
    "                                high_price = high_price / 1000\n",
    "                                low_price = low_price / 1000\n",
    "                                close_price = close_price / 1000\n",
    "                            else:\n",
    "                                # Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ù„Ù‡Ø§ 5 Ø®Ø§Ù†Ø§Øª Ø¹Ø´Ø±ÙŠØ©\n",
    "                                open_price = open_price / 100000\n",
    "                                high_price = high_price / 100000\n",
    "                                low_price = low_price / 100000\n",
    "                                close_price = close_price / 100000\n",
    "                            \n",
    "                            # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø´Ù…Ø¹Ø©\n",
    "                            candle_time = day + timedelta(seconds=time_offset) + timedelta(hours=gmt_offset)\n",
    "                            \n",
    "                            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ù…Ø¹Ø§Ù‹\n",
    "                            formatted_time = candle_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                            \n",
    "                            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ©\n",
    "                            row = [formatted_time, round(open_price, 5), round(high_price, 5),\n",
    "                                   round(low_price, 5), round(close_price, 5), round(volume, 2)]\n",
    "                            all_data.append(row)\n",
    "                        except struct.error as se:\n",
    "                            print(f\"\\nâš ï¸ Struct error: {se} for chunk length {len(chunk)}\")\n",
    "                            continue\n",
    "                            \n",
    "                except lzma.LZMAError as e:\n",
    "                    print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "                    continue\n",
    "            elif r.status_code == 404:\n",
    "                # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total days processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total candles collected: {len(all_data)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù„Ù„Ø´Ù…ÙˆØ¹ Ù…Ø¹ Ø§Ø³Ù… Ù…ÙˆØ­Ø¯\n",
    "    base_filename = f\"{symbol}_candles_{timeframe}_{start.strftime('%Y%m%d')}_{end.strftime('%Y%m%d')}.csv\"\n",
    "    filename = get_safe_filename(path, base_filename)\n",
    "    \n",
    "    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©\n",
    "    final_filename, success, message = safe_file_write(filename, headers, all_data if all_data else None)\n",
    "    \n",
    "    if success:\n",
    "        file_size = os.path.getsize(final_filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(final_filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        if message:\n",
    "            print(f\"â„¹ï¸ Note: {message}\")\n",
    "        if not all_data:\n",
    "            print(\"âš ï¸ No candle data was collected. File contains headers only.\")\n",
    "        \n",
    "        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„\n",
    "        save_download_metadata(symbol, \"Candlestick\", start, end, final_filename, headers, timeframe=timeframe)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ {message}\")\n",
    "        return False\n",
    "\n",
    "# ========== Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª ==========\n",
    "def merge_files():\n",
    "    \"\"\"Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª CSV Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹ Ù†ÙØ³ Ø§Ù„ØªØ±ÙƒÙŠØ¨\"\"\"\n",
    "    metadata = load_download_metadata()\n",
    "    if len(metadata) < 2:\n",
    "        print(\"âŒ Need at least 2 files to merge.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nğŸ“ Available files for merging:\")\n",
    "    for i, item in enumerate(metadata, 1):\n",
    "        print(f\"{i}. {item['filename']} - {item['symbol']} ({item['data_type']})\")\n",
    "        print(f\"   ğŸ“Š Columns: {', '.join(item['columns'])}\")\n",
    "    \n",
    "    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ø¯Ù…Ø¬\n",
    "    selected_files = []\n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"\\nEnter file numbers to merge (comma-separated, e.g. 1,2,3) or 'done': \").strip()\n",
    "            if choice.lower() == 'done':\n",
    "                break\n",
    "            \n",
    "            file_indices = [int(x.strip()) - 1 for x in choice.split(',')]\n",
    "            for idx in file_indices:\n",
    "                if 0 <= idx < len(metadata):\n",
    "                    selected_files.append(metadata[idx])\n",
    "                else:\n",
    "                    print(f\"âŒ Invalid file number: {idx + 1}\")\n",
    "                    \n",
    "            if len(selected_files) >= 2:\n",
    "                break\n",
    "            else:\n",
    "                print(\"âŒ Select at least 2 files to merge.\")\n",
    "                selected_files = []\n",
    "                \n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid input format.\")\n",
    "    \n",
    "    if len(selected_files) < 2:\n",
    "        print(\"âŒ Merge cancelled.\")\n",
    "        return\n",
    "    \n",
    "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
    "    first_columns = selected_files[0]['columns']\n",
    "    incompatible_files = []\n",
    "    \n",
    "    for file_info in selected_files[1:]:\n",
    "        if file_info['columns'] != first_columns:\n",
    "            incompatible_files.append(file_info['filename'])\n",
    "    \n",
    "    if incompatible_files:\n",
    "        print(f\"âŒ Column mismatch detected in files: {', '.join(incompatible_files)}\")\n",
    "        print(f\"Expected columns: {', '.join(first_columns)}\")\n",
    "        for file_info in selected_files:\n",
    "            if file_info['filename'] in incompatible_files:\n",
    "                print(f\"   {file_info['filename']}: {', '.join(file_info['columns'])}\")\n",
    "        return\n",
    "    \n",
    "    # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª\n",
    "    print(f\"\\nğŸ”„ Merging {len(selected_files)} files...\")\n",
    "    merged_data = []\n",
    "    \n",
    "    for file_info in selected_files:\n",
    "        file_path = file_info['full_path']\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"âš ï¸ Warning: File not found: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                header = next(reader)  # ØªØ®Ø·ÙŠ Ø§Ù„Ø±Ø£Ø³\n",
    "                data = list(reader)\n",
    "                merged_data.extend(data)\n",
    "                print(f\"   âœ… Added {len(data)} rows from {file_info['filename']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading {file_info['filename']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not merged_data:\n",
    "        print(\"âŒ No data to merge.\")\n",
    "        return\n",
    "    \n",
    "    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª\n",
    "    try:\n",
    "        merged_data.sort(key=lambda x: datetime.strptime(x[0], \"%Y-%m-%d %H:%M:%S.%f\" if '.' in x[0] else \"%Y-%m-%d %H:%M:%S\"))\n",
    "        print(f\"   âœ… Sorted {len(merged_data)} rows by datetime\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Warning: Could not sort data by datetime: {e}\")\n",
    "    \n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¯Ù…Ø¬\n",
    "    symbols = list(set([f['symbol'] for f in selected_files]))\n",
    "    data_types = list(set([f['data_type'] for f in selected_files]))\n",
    "    \n",
    "    merged_filename = f\"merged_{'_'.join(symbols)}_{'_'.join(data_types)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    merged_path = get_safe_filename(DEFAULT_SAVE_PATH, merged_filename)\n",
    "    \n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¯Ù…Ø¬\n",
    "    final_path, success, message = safe_file_write(merged_path, first_columns, merged_data)\n",
    "    \n",
    "    if success:\n",
    "        file_size = os.path.getsize(final_path) / (1024 * 1024)\n",
    "        print(f\"\\nâœ… Merged file saved: {os.path.abspath(final_path)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        print(f\"ğŸ“Š Total rows: {len(merged_data)}\")\n",
    "        if message:\n",
    "            print(f\"â„¹ï¸ Note: {message}\")\n",
    "        \n",
    "        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¯Ù…Ø¬\n",
    "        start_dates = [datetime.fromisoformat(f['start_date']) for f in selected_files]\n",
    "        end_dates = [datetime.fromisoformat(f['end_date']) for f in selected_files]\n",
    "        \n",
    "        save_download_metadata(\n",
    "            symbol='_'.join(symbols),\n",
    "            data_type='Merged_' + '_'.join(data_types),\n",
    "            start_date=min(start_dates),\n",
    "            end_date=max(end_dates),\n",
    "            filename=final_path,\n",
    "            columns=first_columns\n",
    "        )\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ {message}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool - Enhanced Version\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        main_options = [\"Download New Data\", \"View Previous Downloads\", \"Merge Files\", \"Exit\"]\n",
    "        main_choice = show_menu(main_options, \"Select action\")\n",
    "        \n",
    "        if main_choice == \"View Previous Downloads\":\n",
    "            show_previous_downloads()\n",
    "            \n",
    "        elif main_choice == \"Merge Files\":\n",
    "            merge_files()\n",
    "            \n",
    "        elif main_choice == \"Exit\":\n",
    "            print(\"ğŸ‘‹ Goodbye!\")\n",
    "            exit()\n",
    "            \n",
    "        elif main_choice == \"Download New Data\":\n",
    "            data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "            category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "            symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "            gmt_offset = get_gmt_offset()\n",
    "            start = get_date_input(\"Start date\")\n",
    "            end = get_date_input(\"End date\")\n",
    "\n",
    "            if start > end:\n",
    "                print(\"âŒ End date must be after start date.\")\n",
    "                exit()\n",
    "\n",
    "            print(f\"\\nğŸ“ Configuration Summary:\")\n",
    "            print(f\"   - Symbol: {symbol}\")\n",
    "            print(f\"   - Data type: {data_type}\")\n",
    "            print(f\"   - Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"   - GMT offset: {gmt_offset}\")\n",
    "            print(f\"   - Save path: {DEFAULT_SAVE_PATH}\")\n",
    "\n",
    "            success = False\n",
    "            if data_type == \"Tick\":\n",
    "                price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "                print(f\"   - Price type: {price_type}\")\n",
    "                columns = get_standardized_columns(\"Tick\", price_type)\n",
    "                print(f\"   - Columns: {', '.join(columns)}\")\n",
    "                success = download_tick(symbol, start, end, price_type, gmt_offset)\n",
    "            else:\n",
    "                tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "                tf_code = CANDLE_FRAME_CODES[tf]\n",
    "                print(f\"   - Timeframe: {tf}\")\n",
    "                columns = get_standardized_columns(\"Candlestick\")\n",
    "                print(f\"   - Columns: {', '.join(columns)}\")\n",
    "                success = download_candles(symbol, tf_code, tf, start, end, gmt_offset)\n",
    "\n",
    "            if success:\n",
    "                print(\"\\nâœ… Download completed successfully!\")\n",
    "                print(\"\\nğŸ’¡ Tip: You can now use 'Merge Files' option to combine multiple downloads.\")\n",
    "            else:\n",
    "                print(\"\\nâŒ Download completed with errors!\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâ¹ï¸ Operation interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed6dfdbf-22b6-4f9b-89a9-1993f94fefef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dukascopy Downloader Tool\n",
      "========================================\n",
      "\n",
      "ğŸ”¹ Select data type\n",
      "1. Tick\n",
      "2. Candlestick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select category\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Select symbol\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n",
      "Enter GMT offset (e.g. 0, 2, -5) [default 0]:  0\n",
      "Start date (e.g. 2020-01-01):  2020-01-01\n",
      "End date (e.g. 2020-01-01):  2020-01-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Configuration Summary:\n",
      "   - Symbol: XAUUSD\n",
      "   - Data type: Candlestick\n",
      "   - Date range: 2020-01-01 to 2020-01-01\n",
      "   - GMT offset: 0\n",
      "   - Save path: C:\\Users\\Access\n",
      "\n",
      "ğŸ”¹ Select timeframe\n",
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Timeframe: 1h\n",
      "\n",
      "ğŸ”„ Starting candle download for XAUUSD...\n",
      "ğŸ“… Date range: 2020-01-01 to 2020-01-01\n",
      "â° GMT offset: 0\n",
      "ğŸ“Š Timeframe code: 3600\n",
      "[==================================================] 100.00% | 0.0MB of 0.0MB | ETA: 0h 0m 0s\n",
      "\n",
      "ğŸ“Š Download Summary:\n",
      "   - Total days processed: 1\n",
      "   - Successful downloads: 0\n",
      "   - Total candles collected: 0\n",
      "âš ï¸ No candle data was collected. Creating empty file with headers only.\n",
      "âœ… File saved: C:\\Users\\Access\\XAUUSD_candles_3600_merged.csv\n",
      "ğŸ“¦ Size: 0.00 MB\n",
      "\n",
      "âœ… Download completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ==========\n",
    "DEFAULT_SAVE_PATH = os.getcwd()\n",
    "CATEGORIES = {\n",
    "    \"Metals\": [\"XAUUSD\", \"XAGUSD\"],\n",
    "    \"Forex\": [\"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"],\n",
    "    \"Indices\": [\"US500\", \"NAS100\", \"GER30\"]\n",
    "}\n",
    "TIMEFRAMES = [\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "CANDLE_FRAME_CODES = {\n",
    "    \"1m\": \"60\", \"5m\": \"300\", \"15m\": \"900\", \"1h\": \"3600\", \"4h\": \"14400\", \"1d\": \"86400\"\n",
    "}\n",
    "\n",
    "# ========== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ==========\n",
    "def show_menu(options, title):\n",
    "    print(f\"\\nğŸ”¹ {title}\")\n",
    "    for i, opt in enumerate(options):\n",
    "        print(f\"{i + 1}. {opt}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \")) - 1\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            else:\n",
    "                print(\"âŒ Invalid number.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Please enter a number.\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (e.g. 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Invalid date format.\")\n",
    "\n",
    "def get_gmt_offset():\n",
    "    try:\n",
    "        val = input(\"Enter GMT offset (e.g. 0, 2, -5) [default 0]: \").strip()\n",
    "        return int(val) if val else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def format_seconds(seconds):\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ Ø¥Ù„Ù‰ ØµÙŠØºØ© (Ø³Ø§Ø¹Ø§Øª - Ø¯Ù‚Ø§Ø¦Ù‚ - Ø«ÙˆØ§Ù†ÙŠ)\n",
    "    hrs, rem = divmod(int(seconds), 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    return f\"{hrs}h {mins}m {secs}s\"\n",
    "\n",
    "# ========== Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ==========\n",
    "def progress_bar(progress, total, start_time, downloaded_bytes, estimated_total_bytes):\n",
    "    percent = 100 * (progress / total)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = progress / elapsed if elapsed > 0 else 0\n",
    "    remaining = (total - progress) / rate if rate > 0 else 0\n",
    "    downloaded_mb = downloaded_bytes / (1024 * 1024)\n",
    "    total_est_mb = estimated_total_bytes / (1024 * 1024)\n",
    "    bar = '=' * int(percent / 2) + ' ' * (50 - int(percent / 2))\n",
    "    sys.stdout.write(f\"\\r[{bar}] {percent:.2f}% | {downloaded_mb:.1f}MB of {total_est_mb:.1f}MB | ETA: {format_seconds(remaining)}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Tick ==========\n",
    "def download_tick(symbol, start, end, price_type, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_ticks = []\n",
    "    total_hours = ((end - start).days + 1) * 24\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_hours * 30 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting tick download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    \n",
    "    for day in (start + timedelta(days=i) for i in range((end - start).days + 1)):\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                r = requests.get(url, timeout=15)\n",
    "                if r.status_code == 200 and r.content:\n",
    "                    successful_downloads += 1\n",
    "                    downloaded_bytes += len(r.content)\n",
    "                    try:\n",
    "                        data = lzma.decompress(r.content)\n",
    "                        for i in range(0, len(data), 20):\n",
    "                            chunk = data[i:i+20]\n",
    "                            if len(chunk) < 20: \n",
    "                                continue\n",
    "                            t_off, ask, bid, ask_vol, bid_vol = struct.unpack(\">IIfff\", chunk)\n",
    "                            tick_time = day + timedelta(hours=hour, milliseconds=t_off)\n",
    "                            tick_time += timedelta(hours=gmt_offset)\n",
    "                            row = [tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]]\n",
    "                            \n",
    "                            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                                row += [round(bid, 5), round(bid_vol, 2)]\n",
    "                            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                                row += [round(ask, 5), round(ask_vol, 2)]\n",
    "                            else:\n",
    "                                row += [round(bid, 5), round(ask, 5), round(bid_vol, 2), round(ask_vol, 2)]\n",
    "                            all_ticks.append(row)\n",
    "                    except lzma.LZMAError as e:\n",
    "                        print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                        continue\n",
    "                elif r.status_code == 404:\n",
    "                    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ®/Ø§Ù„Ø³Ø§Ø¹Ø© (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')} {hour}h\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')} {hour}h: {e}\")\n",
    "                continue\n",
    "                \n",
    "            count += 1\n",
    "            progress_bar(count, total_hours, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total hours processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total ticks collected: {len(all_ticks)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø©\n",
    "    filename = os.path.join(path, f\"{symbol}_tick_merged.csv\")\n",
    "    try:\n",
    "        with open(filename, \"w\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if price_type == \"Bid ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"bid\", \"bid_vol\"])\n",
    "            elif price_type == \"Ask ÙÙ‚Ø·\":\n",
    "                writer.writerow([\"time\", \"ask\", \"ask_vol\"])\n",
    "            else:\n",
    "                writer.writerow([\"time\", \"bid\", \"ask\", \"bid_vol\", \"ask_vol\"])\n",
    "            \n",
    "            if all_ticks:\n",
    "                all_ticks.sort(key=lambda x: x[0])\n",
    "                writer.writerows(all_ticks)\n",
    "            else:\n",
    "                print(\"âš ï¸ No tick data was collected. Creating empty file with headers only.\")\n",
    "        \n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Candlestick ==========\n",
    "def download_candles(symbol, tf_code, start, end, gmt_offset):\n",
    "    path = DEFAULT_SAVE_PATH\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    all_data = []\n",
    "    total_days = (end - start).days + 1\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    downloaded_bytes = 0\n",
    "    estimated_total_bytes = total_days * 10 * 1024\n",
    "    successful_downloads = 0\n",
    "\n",
    "    print(f\"\\nğŸ”„ Starting candle download for {symbol}...\")\n",
    "    print(f\"ğŸ“… Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"â° GMT offset: {gmt_offset}\")\n",
    "    print(f\"ğŸ“Š Timeframe code: {tf_code}\")\n",
    "\n",
    "    for day in (start + timedelta(days=i) for i in range(total_days)):\n",
    "        url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{day.year}/{day.month - 1:02d}/{day.day:02d}/{tf_code}_candles.bi5\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                successful_downloads += 1\n",
    "                downloaded_bytes += len(r.content)\n",
    "                try:\n",
    "                    data = lzma.decompress(r.content)\n",
    "                    for i in range(0, len(data), 24):  # Changed from 20 to 24 bytes for candles\n",
    "                        chunk = data[i:i+24]\n",
    "                        if len(chunk) < 24: \n",
    "                            continue\n",
    "                        utc_offset, open_, high, low, close, vol = struct.unpack(\">IIffff\", chunk[:24])\n",
    "                        candle_time = day + timedelta(seconds=utc_offset) + timedelta(hours=gmt_offset)\n",
    "                        row = [candle_time.strftime(\"%Y-%m-%d %H:%M:%S\"), round(open_, 5), round(high, 5),\n",
    "                               round(low, 5), round(close, 5), round(vol, 2)]\n",
    "                        all_data.append(row)\n",
    "                except lzma.LZMAError as e:\n",
    "                    print(f\"\\nâš ï¸ Warning: Failed to decompress data for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "                    continue\n",
    "            elif r.status_code == 404:\n",
    "                # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø·Ø¨ÙŠØ¹ÙŠ)\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸ Warning: HTTP {r.status_code} for {day.strftime('%Y-%m-%d')}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Network error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Warning: Unexpected error for {day.strftime('%Y-%m-%d')}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "        progress_bar(count, total_days, start_time, downloaded_bytes, estimated_total_bytes)\n",
    "\n",
    "    print(f\"\\n\\nğŸ“Š Download Summary:\")\n",
    "    print(f\"   - Total days processed: {count}\")\n",
    "    print(f\"   - Successful downloads: {successful_downloads}\")\n",
    "    print(f\"   - Total candles collected: {len(all_data)}\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø©\n",
    "    filename = os.path.join(path, f\"{symbol}_candles_{tf_code}_merged.csv\")\n",
    "    try:\n",
    "        with open(filename, \"w\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "            \n",
    "            if all_data:\n",
    "                all_data.sort(key=lambda x: x[0])\n",
    "                writer.writerows(all_data)\n",
    "            else:\n",
    "                print(\"âš ï¸ No candle data was collected. Creating empty file with headers only.\")\n",
    "        \n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"âœ… File saved: {os.path.abspath(filename)}\")\n",
    "        print(f\"ğŸ“¦ Size: {file_size:.2f} MB\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "# ========== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nğŸ“Š Dukascopy Downloader Tool\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    try:\n",
    "        data_type = show_menu([\"Tick\", \"Candlestick\"], \"Select data type\")\n",
    "        category = show_menu(list(CATEGORIES.keys()), \"Select category\")\n",
    "        symbol = show_menu(CATEGORIES[category], \"Select symbol\")\n",
    "        gmt_offset = get_gmt_offset()\n",
    "        start = get_date_input(\"Start date\")\n",
    "        end = get_date_input(\"End date\")\n",
    "\n",
    "        if start > end:\n",
    "            print(\"âŒ End date must be after start date.\")\n",
    "            exit()\n",
    "\n",
    "        print(f\"\\nğŸ“ Configuration Summary:\")\n",
    "        print(f\"   - Symbol: {symbol}\")\n",
    "        print(f\"   - Data type: {data_type}\")\n",
    "        print(f\"   - Date range: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   - GMT offset: {gmt_offset}\")\n",
    "        print(f\"   - Save path: {DEFAULT_SAVE_PATH}\")\n",
    "\n",
    "        success = False\n",
    "        if data_type == \"Tick\":\n",
    "            price_type = show_menu([\"Bid ÙÙ‚Ø·\", \"Ask ÙÙ‚Ø·\", \"ÙƒÙ„Ø§Ù‡Ù…Ø§\"], \"Price type\")\n",
    "            print(f\"   - Price type: {price_type}\")\n",
    "            success = download_tick(symbol, start, end, price_type, gmt_offset)\n",
    "        else:\n",
    "            tf = show_menu(TIMEFRAMES, \"Select timeframe\")\n",
    "            tf_code = CANDLE_FRAME_CODES[tf]\n",
    "            print(f\"   - Timeframe: {tf}\")\n",
    "            success = download_candles(symbol, tf_code, start, end, gmt_offset)\n",
    "\n",
    "        if success:\n",
    "            print(\"\\nâœ… Download completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Download completed with errors!\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâ¹ï¸ Download interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73c366ad-fa45-4a0f-b072-33f73e516f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¡ Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§ØªÙ‡Ø§:\n",
      "1. XAUUSD\n",
      "2. EURUSD\n",
      "3. GBPUSD\n",
      "4. USDJPY\n",
      "5. USDCHF\n",
      "6. AUDUSD\n",
      "7. NZDUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Ø£Ø¯Ø®Ù„ Ø±Ù‚Ù… Ø§Ù„Ø£Ø¯Ø§Ø©:  2\n",
      "ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ù…Ø«Ø§Ù„: 2020-01-01):  2020-01-01\n",
      "ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Ù…Ø«Ø§Ù„: 2020-01-01):  2020-01-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â¬‡ï¸ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª EURUSD Ù…Ù† 2020-01-01 Ø¥Ù„Ù‰ 2020-01-01...\n",
      "\n",
      "âœ… ØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ… 2020-01-01 ÙÙŠ EURUSD_2020-01-01.csv\n",
      "\n",
      "ğŸ§© Ø¬Ø§Ø±ÙŠ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª...\n",
      "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¯Ù…Ø¬ ÙÙŠ: output\\EURUSD_merged_20200101_to_20200101.csv\n",
      "\n",
      "âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¯Ù…Ø¬.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd  # Ù…ÙƒØªØ¨Ø© Ù„Ø¯Ù…Ø¬ ÙˆÙØ±Ø² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "symbols = [\n",
    "    \"XAUUSD\", \"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"\n",
    "]\n",
    "\n",
    "def show_symbol_menu():\n",
    "    print(\"ğŸŸ¡ Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§ØªÙ‡Ø§:\")\n",
    "    for i, sym in enumerate(symbols):\n",
    "        print(f\"{i + 1}. {sym}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"ğŸ”¢ Ø£Ø¯Ø®Ù„ Ø±Ù‚Ù… Ø§Ù„Ø£Ø¯Ø§Ø©: \")) - 1\n",
    "            if 0 <= choice < len(symbols):\n",
    "                return symbols[choice]\n",
    "            else:\n",
    "                print(\"âŒ Ø±Ù‚Ù… ØºÙŠØ± ØµØ­ÙŠØ­. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù….\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (Ù…Ø«Ø§Ù„: 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Ø§Ù„ØªØ§Ø±ÙŠØ® ØºÙŠØ± ØµØ­ÙŠØ­. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\")\n",
    "\n",
    "def download_tick_data(symbol, start_date, end_date, save_path=\"output\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    current = start_date\n",
    "\n",
    "    while current <= end_date:\n",
    "        daily_ticks = []\n",
    "\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{current.year}/{current.month - 1:02d}/{current.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200 and response.content:\n",
    "                    decompressed = lzma.decompress(response.content)\n",
    "                    for i in range(0, len(decompressed), 20):\n",
    "                        chunk = decompressed[i:i+20]\n",
    "                        if len(chunk) < 20:\n",
    "                            continue\n",
    "                        timestamp_offset, ask, bid, ask_vol, bid_vol = struct.unpack(\">IIfff\", chunk)\n",
    "                        tick_time = current + timedelta(hours=hour, milliseconds=timestamp_offset)\n",
    "                        daily_ticks.append([\n",
    "                            tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3],\n",
    "                            round(bid, 5),\n",
    "                            round(ask, 5),\n",
    "                            round(bid_vol, 2),\n",
    "                            round(ask_vol, 2)\n",
    "                        ])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¹Ø© {hour}: {e}\")\n",
    "\n",
    "        if daily_ticks:\n",
    "            filename = f\"{symbol}_{current.strftime('%Y-%m-%d')}.csv\"\n",
    "            with open(os.path.join(save_path, filename), \"w\", newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"gmt_time\", \"bid\", \"ask\", \"bid_vol\", \"ask_vol\"])\n",
    "                writer.writerows(daily_ticks)\n",
    "            print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ… {current.date()} ÙÙŠ {filename}\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {current.date()}\")\n",
    "\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "def merge_csv_files(symbol, start_date, end_date, save_path=\"output\"):\n",
    "    print(\"\\nğŸ§© Ø¬Ø§Ø±ÙŠ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª...\")\n",
    "    all_files = [\n",
    "        os.path.join(save_path, f) for f in os.listdir(save_path)\n",
    "        if f.startswith(symbol) and f.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ù„ÙØ§Øª CSV Ù„Ù„Ø¯Ù…Ø¬.\")\n",
    "        return\n",
    "\n",
    "    df_list = []\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file}: {e}\")\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"âŒ Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ù‚Ø±Ø§Ø¡Ø© Ø£ÙŠ Ù…Ù„ÙØ§Øª.\")\n",
    "        return\n",
    "\n",
    "    merged_df = pd.concat(df_list)\n",
    "    merged_df.sort_values(by=\"gmt_time\", inplace=True)\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    merged_filename = f\"{symbol}_merged_{start_date.strftime('%Y%m%d')}_to_{end_date.strftime('%Y%m%d')}.csv\"\n",
    "    merged_path = os.path.join(save_path, merged_filename)\n",
    "    merged_df.to_csv(merged_path, index=False)\n",
    "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¯Ù…Ø¬ ÙÙŠ: {merged_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# ğŸš€ Ø§Ù„ØªØ´ØºÙŠÙ„\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    symbol = show_symbol_menu()\n",
    "    start = get_date_input(\"ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©\")\n",
    "    end = get_date_input(\"ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ©\")\n",
    "\n",
    "    if end < start:\n",
    "        print(\"âŒ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨Ø¹Ø¯ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©.\")\n",
    "    else:\n",
    "        print(f\"\\nâ¬‡ï¸ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ù…Ù† {start.date()} Ø¥Ù„Ù‰ {end.date()}...\\n\")\n",
    "        download_tick_data(symbol, start, end)\n",
    "        merge_csv_files(symbol, start, end)\n",
    "        print(\"\\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¯Ù…Ø¬.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654d476a-3f39-4ea1-8965-04c78d57eb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¡ Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§ØªÙ‡Ø§:\n",
      "1. XAUUSD\n",
      "2. EURUSD\n",
      "3. GBPUSD\n",
      "4. USDJPY\n",
      "5. USDCHF\n",
      "6. AUDUSD\n",
      "7. NZDUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Ø£Ø¯Ø®Ù„ Ø±Ù‚Ù… Ø§Ù„Ø£Ø¯Ø§Ø©:  2\n",
      "ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ù…Ø«Ø§Ù„: 2020-01-01):  2020-01-01\n",
      "ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Ù…Ø«Ø§Ù„: 2020-01-01):  2020-01-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â¬‡ï¸ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª EURUSD Ù…Ù† 2020-01-01 Ø¥Ù„Ù‰ 2020-01-03...\n",
      "\n",
      "âœ… ØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ… 2020-01-01 ÙÙŠ EURUSD_2020-01-01.csv\n",
      "âœ… ØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ… 2020-01-02 ÙÙŠ EURUSD_2020-01-02.csv\n",
      "âœ… ØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ… 2020-01-03 ÙÙŠ EURUSD_2020-01-03.csv\n",
      "\n",
      "ğŸ§© Ø¬Ø§Ø±ÙŠ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª...\n",
      "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¯Ù…Ø¬ ÙÙŠ: output\\EURUSD_merged_20200101_to_20200103.csv\n",
      "\n",
      "âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¯Ù…Ø¬.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lzma\n",
    "import struct\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd  # Ù…ÙƒØªØ¨Ø© Ù„Ø¯Ù…Ø¬ ÙˆÙØ±Ø² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "symbols = [\n",
    "    \"XAUUSD\", \"EURUSD\", \"GBPUSD\", \"USDJPY\", \"USDCHF\", \"AUDUSD\", \"NZDUSD\"\n",
    "]\n",
    "\n",
    "def show_symbol_menu():\n",
    "    print(\"ğŸŸ¡ Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§ØªÙ‡Ø§:\")\n",
    "    for i, sym in enumerate(symbols):\n",
    "        print(f\"{i + 1}. {sym}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"ğŸ”¢ Ø£Ø¯Ø®Ù„ Ø±Ù‚Ù… Ø§Ù„Ø£Ø¯Ø§Ø©: \")) - 1\n",
    "            if 0 <= choice < len(symbols):\n",
    "                return symbols[choice]\n",
    "            else:\n",
    "                print(\"âŒ Ø±Ù‚Ù… ØºÙŠØ± ØµØ­ÙŠØ­. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù….\")\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            return datetime.strptime(input(f\"{prompt} (Ù…Ø«Ø§Ù„: 2020-01-01): \"), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ Ø§Ù„ØªØ§Ø±ÙŠØ® ØºÙŠØ± ØµØ­ÙŠØ­. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\")\n",
    "\n",
    "def download_tick_data(symbol, start_date, end_date, save_path=\"output\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    current = start_date\n",
    "\n",
    "    while current <= end_date:\n",
    "        daily_ticks = []\n",
    "\n",
    "        for hour in range(24):\n",
    "            url = f\"https://datafeed.dukascopy.com/datafeed/{symbol}/{current.year}/{current.month - 1:02d}/{current.day:02d}/{hour}h_ticks.bi5\"\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200 and response.content:\n",
    "                    decompressed = lzma.decompress(response.content)\n",
    "                    for i in range(0, len(decompressed), 20):\n",
    "                        chunk = decompressed[i:i+20]\n",
    "                        if len(chunk) < 20:\n",
    "                            continue\n",
    "                        timestamp_offset, ask, bid, ask_vol, bid_vol = struct.unpack(\">IIfff\", chunk)\n",
    "                        tick_time = current + timedelta(hours=hour, milliseconds=timestamp_offset)\n",
    "                        daily_ticks.append([\n",
    "                            tick_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3],\n",
    "                            round(ask, 5),\n",
    "                            round(bid, 5),\n",
    "                            round(ask_vol, 2),\n",
    "                            round(bid_vol, 2)\n",
    "                        ])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¹Ø© {hour}: {e}\")\n",
    "\n",
    "        if daily_ticks:\n",
    "            filename = f\"{symbol}_{current.strftime('%Y-%m-%d')}.csv\"\n",
    "            with open(os.path.join(save_path, filename), \"w\", newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"Gmt time\", \"Ask\", \"Bid\", \"AskVolume\", \"BidVolume\"])\n",
    "                writer.writerows(daily_ticks)\n",
    "            print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ… {current.date()} ÙÙŠ {filename}\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {current.date()}\")\n",
    "\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "def merge_csv_files(symbol, start_date, end_date, save_path=\"output\"):\n",
    "    print(\"\\nğŸ§© Ø¬Ø§Ø±ÙŠ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª...\")\n",
    "    all_files = [\n",
    "        os.path.join(save_path, f) for f in os.listdir(save_path)\n",
    "        if f.startswith(symbol) and f.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ù„ÙØ§Øª CSV Ù„Ù„Ø¯Ù…Ø¬.\")\n",
    "        return\n",
    "\n",
    "    df_list = []\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file}: {e}\")\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"âŒ Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ù‚Ø±Ø§Ø¡Ø© Ø£ÙŠ Ù…Ù„ÙØ§Øª.\")\n",
    "        return\n",
    "\n",
    "    merged_df = pd.concat(df_list)\n",
    "    merged_df.sort_values(by=\"Gmt time\", inplace=True)\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    merged_filename = f\"{symbol}_merged_{start_date.strftime('%Y%m%d')}_to_{end_date.strftime('%Y%m%d')}.csv\"\n",
    "    merged_path = os.path.join(save_path, merged_filename)\n",
    "    merged_df.to_csv(merged_path, index=False)\n",
    "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¯Ù…Ø¬ ÙÙŠ: {merged_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# ğŸš€ Ø§Ù„ØªØ´ØºÙŠÙ„\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    symbol = show_symbol_menu()\n",
    "    start = get_date_input(\"ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©\")\n",
    "    end = get_date_input(\"ğŸ“… Ø£Ø¯Ø®Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ©\")\n",
    "\n",
    "    if end < start:\n",
    "        print(\"âŒ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨Ø¹Ø¯ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©.\")\n",
    "    else:\n",
    "        print(f\"\\nâ¬‡ï¸ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª {symbol} Ù…Ù† {start.date()} Ø¥Ù„Ù‰ {end.date()}...\\n\")\n",
    "        download_tick_data(symbol, start, end)\n",
    "        merge_csv_files(symbol, start, end)\n",
    "        print(\"\\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¯Ù…Ø¬.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab3aadeb-57b9-4727-bd8e-de0297769048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select main category:\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter category number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected category: Metals\n",
      "\n",
      "Select instrument:\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter instrument number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected instrument: XAUUSD\n",
      "\n",
      "1. Candlestick\n",
      "2. Tick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose data type (number):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select timeframe:  3\n",
      "Enter start date (YYYY-MM-DD):  2020-01-01\n",
      "Enter end date (YYYY-MM-DD):  2020-01-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.dukascopy.com/datafeed/XAUUSD/15m/2020/01/01/15m.bi5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XAUUSD_20200101.csv: 4.52kiB [00:00, 9.39kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.dukascopy.com/datafeed/XAUUSD/15m/2020/01/02/15m.bi5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XAUUSD_20200102.csv: 4.52kiB [00:00, 813kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.dukascopy.com/datafeed/XAUUSD/15m/2020/01/03/15m.bi5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XAUUSD_20200103.csv: 4.52kiB [00:00, ?iB/s]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 11, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 135\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles saved to directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 128\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Merge files\u001b[39;00m\n\u001b[0;32m    127\u001b[0m merged_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_merged.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 128\u001b[0m \u001b[43mmerge_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownloaded_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles saved to directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 76\u001b[0m, in \u001b[0;36mmerge_csv\u001b[1;34m(files, output_path)\u001b[0m\n\u001b[0;32m     74\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m---> 76\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     78\u001b[0m all_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 11, saw 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "BASE_URL = 'https://www.dukascopy.com/datafeed'\n",
    "\n",
    "# Mapping of categories to URL paths (example)\n",
    "CATEGORIES = {\n",
    "    'Metals': ['XAUUSD', 'XAGUSD'],\n",
    "    'Forex': ['EURUSD', 'GBPUSD', 'USDJPY'],\n",
    "    'Indices': ['DAX', 'SP500'],\n",
    "    # Add more...\n",
    "}\n",
    "HISTORICAL_PATH = 'widgets/quotes/historical_data_feed'\n",
    "\n",
    "\n",
    "def select_option(options, prompt_text):\n",
    "    \"\"\"\n",
    "    Display a numbered list of options and return the selected item.\n",
    "    \"\"\"\n",
    "    for idx, opt in enumerate(options, 1):\n",
    "        print(f\"{idx}. {opt}\")\n",
    "    while True:\n",
    "        choice = input(prompt_text)\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(options):\n",
    "            return options[int(choice) - 1]\n",
    "        print(\"Invalid selection, please try again.\")\n",
    "\n",
    "\n",
    "def build_url(symbol, data_type, start_dt, end_dt, timeframe=None):\n",
    "    \"\"\"\n",
    "    Construct the download URL for dukascopy data.\n",
    "    \"\"\"\n",
    "    start_str = start_dt.strftime('%Y/%m/%d')\n",
    "    end_str = end_dt.strftime('%Y/%m/%d')\n",
    "    if data_type == 'Tick':\n",
    "        path = f\"{symbol}/{start_dt.year}/{start_dt.strftime('%m')}/{start_dt.strftime('%d')}/ticks.bi5\"\n",
    "    else:\n",
    "        path = f\"{symbol}/{timeframe}/{start_dt.year}/{start_dt.strftime('%m')}/{start_dt.strftime('%d')}/{timeframe}.bi5\"\n",
    "    return f\"{BASE_URL}/{path}\"  # Adjust as per actual API\n",
    "\n",
    "\n",
    "def download_file(url, dest_path):\n",
    "    \"\"\"\n",
    "    Download a file with progress bar showing time remaining, speed, and size.\n",
    "    \"\"\"\n",
    "    r = requests.get(url, stream=True)\n",
    "    total_size = int(r.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "    wrote = 0\n",
    "    start = time.time()\n",
    "    with open(dest_path, 'wb') as f, tqdm(\n",
    "        total=total_size, unit='iB', unit_scale=True,\n",
    "        desc=os.path.basename(dest_path)\n",
    "    ) as bar:\n",
    "        for data in r.iter_content(block_size):\n",
    "            f.write(data)\n",
    "            wrote += len(data)\n",
    "            bar.update(len(data))\n",
    "    if total_size != 0 and wrote != total_size:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "\n",
    "def merge_csv(files, output_path):\n",
    "    \"\"\"\n",
    "    Read multiple CSVs, concatenate and sort by Gmt time then save.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "    all_df = pd.concat(dfs, ignore_index=True)\n",
    "    all_df['Gmt time'] = pd.to_datetime(all_df['Gmt time'])\n",
    "    all_df = all_df.sort_values('Gmt time')\n",
    "    all_df.to_csv(output_path, index=False)\n",
    "    print(f\"Merged file saved to: {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Select main category:\")\n",
    "    category = select_option(list(CATEGORIES.keys()), \"Enter category number: \")\n",
    "    symbols = CATEGORIES[category]\n",
    "    print(f\"Selected category: {category}\\n\")\n",
    "\n",
    "    print(\"Select instrument:\")\n",
    "    symbol = select_option(symbols, \"Enter instrument number: \")\n",
    "    print(f\"Selected instrument: {symbol}\\n\")\n",
    "\n",
    "    # Choose data type\n",
    "    data_type = select_option(['Candlestick', 'Tick'], \"Choose data type (number): \")\n",
    "    timeframe = None\n",
    "    if data_type == 'Candlestick':\n",
    "        # Available timeframes\n",
    "        tfs = ['1m', '5m', '15m', '1h', '4h', '1d']\n",
    "        timeframe = select_option(tfs, \"Select timeframe: \")\n",
    "    else:\n",
    "        ticks = input(\"Enter number of ticks per file: \")\n",
    "\n",
    "    # Date selection\n",
    "    start_date = input(\"Enter start date (YYYY-MM-DD): \")\n",
    "    end_date = input(\"Enter end date (YYYY-MM-DD): \")\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Create output dir\n",
    "    out_dir = os.path.join(os.getcwd(), 'downloads', symbol)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    downloaded_files = []\n",
    "    current = start_dt\n",
    "    while current <= end_dt:\n",
    "        url = build_url(symbol, data_type, current, end_dt, timeframe)\n",
    "        date_str = current.strftime('%Y%m%d')\n",
    "        out_file = os.path.join(out_dir, f\"{symbol}_{date_str}.csv\")\n",
    "        print(f\"Downloading: {url}\")\n",
    "        download_file(url, out_file)\n",
    "        downloaded_files.append(out_file)\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "    # Merge files\n",
    "    merged_path = os.path.join(out_dir, f\"{symbol}_{start_date}_{end_date}_merged.csv\")\n",
    "    merge_csv(downloaded_files, merged_path)\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "    print(f\"Files saved to directory: {out_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5708e9c3-b734-4dd6-abdc-3ef1c04313f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select main category:\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter category number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected category: Metals\n",
      "\n",
      "Select instrument:\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter instrument number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected instrument: XAUUSD\n",
      "\n",
      "1. Candlestick\n",
      "2. Tick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose data type (number):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select timeframe:  3\n",
      "Enter start date (YYYY-MM-DD):  2020-01-01\n",
      "Enter end date (YYYY-MM-DD):  2020-01-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading 2020-01-01...\n",
      "âš ï¸ ÙØ´Ù„ ØªÙ†Ø²ÙŠÙ„ XAUUSD_20200101.bi5: ØªØ¹Ø°Ø± Ù…Ø¹Ø±ÙØ© Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø®Ø§Ø¯Ù….\n",
      "\n",
      "Downloading 2020-01-02...\n",
      "âš ï¸ ÙØ´Ù„ ØªÙ†Ø²ÙŠÙ„ XAUUSD_20200102.bi5: ØªØ¹Ø°Ø± Ù…Ø¹Ø±ÙØ© Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø®Ø§Ø¯Ù….\n",
      "\n",
      "Downloading 2020-01-03...\n",
      "âš ï¸ ÙØ´Ù„ ØªÙ†Ø²ÙŠÙ„ XAUUSD_20200103.bi5: ØªØ¹Ø°Ø± Ù…Ø¹Ø±ÙØ© Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø®Ø§Ø¯Ù….\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 167\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll files stored in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 162\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m merged_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_merged.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m merged_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, merged_name)\n\u001b[1;32m--> 162\u001b[0m \u001b[43mmerge_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll files stored in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 98\u001b[0m, in \u001b[0;36mmerge_csv\u001b[1;34m(csv_files, out_path)\u001b[0m\n\u001b[0;32m     96\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(f, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGmt time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     97\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m---> 98\u001b[0m all_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m all_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGmt time\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    100\u001b[0m all_df\u001b[38;5;241m.\u001b[39mto_csv(out_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import lzma\n",
    "import struct\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "BASE_URL = 'https://www.dukascopy.com/datafeed'\n",
    "# Ù…Ø«Ø§Ù„ Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø£Ø¯ÙˆØ§ØªØ› Ø£Ø¶Ù Ù…Ø§ ØªØ±ÙŠØ¯:\n",
    "CATEGORIES = {\n",
    "    'Metals': ['XAUUSD', 'XAGUSD'],\n",
    "    'Forex' : ['EURUSD', 'GBPUSD', 'USDJPY', 'USDCHF', 'AUDUSD'],\n",
    "    'Indices': ['DAX', 'SP500'],\n",
    "}\n",
    "# Ø§Ù„Ø£Ø·Ø± Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„ÙŠØ§Ø¨Ø§Ù†ÙŠØ©\n",
    "TIMEFRAMES = ['1m', '5m', '15m', '1h', '4h', '1d']\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def select_option(options, prompt_text):\n",
    "    \"\"\"Ø¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±.\"\"\"\n",
    "    for idx, opt in enumerate(options, 1):\n",
    "        print(f\"{idx}. {opt}\")\n",
    "    while True:\n",
    "        choice = input(prompt_text).strip()\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(options):\n",
    "            return options[int(choice) - 1]\n",
    "        print(\"Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ§Ù„Ø­ØŒ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\")\n",
    "\n",
    "def build_bi5_url(symbol, data_type, date_dt, timeframe=None):\n",
    "    \"\"\"\n",
    "    Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ù„Ù…Ù„Ù .bi5:\n",
    "    - Tick: symbol/YYYY/MM/DD/ticks.bi5\n",
    "    - Candlestick: symbol/{tf}/YYYY/MM/DD/{tf}.bi5\n",
    "    \"\"\"\n",
    "    y, m, d = date_dt.year, f\"{date_dt.month:02}\", f\"{date_dt.day:02}\"\n",
    "    if data_type == 'Tick':\n",
    "        path = f\"{symbol}/{y}/{m}/{d}/ticks.bi5\"\n",
    "    else:\n",
    "        path = f\"{symbol}/{timeframe}/{y}/{m}/{d}/{timeframe}.bi5\"\n",
    "    return f\"{BASE_URL}/{path}\"\n",
    "\n",
    "def download_bi5(url, dest_path):\n",
    "    \"\"\"ØªÙ†Ø²ÙŠÙ„ Ù…Ù„Ù bi5 Ù…Ø¹ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… ÙŠÙˆØ¶Ø­ Ø§Ù„Ø­Ø¬Ù… ÙˆØ§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ.\"\"\"\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"ØªØ¹Ø°Ø± Ù…Ø¹Ø±ÙØ© Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø®Ø§Ø¯Ù….\")\n",
    "    with open(dest_path, 'wb') as f, tqdm(\n",
    "        total=total, unit='iB', unit_scale=True,\n",
    "        desc=os.path.basename(dest_path),\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    ) as bar:\n",
    "        for chunk in resp.iter_content(1024):\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "def bi5_to_csv(bi5_path, csv_path, timeframe):\n",
    "    \"\"\"\n",
    "    ÙÙƒ Ø¶ØºØ· .bi5 ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ CSV Ø¨Ø¹Ù†Ø§ÙˆÙŠÙ†:\n",
    "    ['Gmt time','Open','High','Low','Close','Volume']\n",
    "    \"\"\"\n",
    "    # Ø­ÙˆÙ‘Ù„ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¥Ù„Ù‰ Ù…ÙŠÙ„ÙŠØ«Ø§Ù†ÙŠØ©\n",
    "    tf_ms = {\n",
    "        '1m': 60_000, '5m': 5*60_000, '15m': 15*60_000,\n",
    "        '1h': 3600_000, '4h': 4*3600_000, '1d': 86400_000\n",
    "    }[timeframe]\n",
    "    \n",
    "    with lzma.open(bi5_path) as fin, open(csv_path, 'w', newline='') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow(['Gmt time','Open','High','Low','Close','Volume'])\n",
    "        record_size = 8 + 5*4  # 8 bytes Ù„Ù„ÙˆÙ‚Øª + 5Ã—4 bytes Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "        while True:\n",
    "            chunk = fin.read(record_size)\n",
    "            if len(chunk) < record_size:\n",
    "                break\n",
    "            t_ms, o, h, l, c, v = struct.unpack('>Qffffi', chunk)\n",
    "            t_str = datetime.utcfromtimestamp(t_ms/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            writer.writerow([t_str, o, h, l, c, v])\n",
    "\n",
    "def merge_csv(csv_files, out_path):\n",
    "    \"\"\"Ø¯Ù…Ø¬ Ø¹Ø¯Ø© CSV ÙØ±Ø¹ÙŠØ© ÙØ±Ø²Ù‹Ø§ Ø­Ø³Ø¨ 'Gmt time' Ø«Ù… Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©.\"\"\"\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(f, parse_dates=['Gmt time'])\n",
    "        dfs.append(df)\n",
    "    all_df = pd.concat(dfs, ignore_index=True)\n",
    "    all_df.sort_values('Gmt time', inplace=True)\n",
    "    all_df.to_csv(out_path, index=False)\n",
    "    print(f\"\\nâœ”ï¸ ØªÙ… Ø§Ù„Ø¯Ù…Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {out_path}\")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def main():\n",
    "    # 1) Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØªØµÙ†ÙŠÙ Ø«Ù… Ø§Ù„Ø£Ø¯Ø§Ø©\n",
    "    print(\"Select main category:\")\n",
    "    category = select_option(list(CATEGORIES.keys()), \"Enter category number: \")\n",
    "    print(f\"Selected category: {category}\\n\")\n",
    "\n",
    "    print(\"Select instrument:\")\n",
    "    symbol = select_option(CATEGORIES[category], \"Enter instrument number: \")\n",
    "    print(f\"Selected instrument: {symbol}\\n\")\n",
    "\n",
    "    # 2) Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: Candlestick Ø£Ùˆ Tick\n",
    "    data_type = select_option(['Candlestick', 'Tick'], \"Choose data type (number): \")\n",
    "    timeframe = None\n",
    "    if data_type == 'Candlestick':\n",
    "        timeframe = select_option(TIMEFRAMES, \"Select timeframe: \")\n",
    "    else:\n",
    "        # Ù†Ø³ØªØ®Ø¯Ù… Ø¥Ø·Ø§Ø± Ø²Ù…Ù†ÙŠ ÙˆÙ‡Ù…ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ bi5 Ø¥Ù„Ù‰ CSV Ø¨Ù€ÙÙˆØ§ØµÙ„ Ø²Ù…Ù†ÙŠØ© Ù…ØªÙ†Ø§Ù‡ÙŠØ©\n",
    "        timeframe = 'tick'  \n",
    "\n",
    "    # 3) ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ\n",
    "    start_date = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
    "    end_date   = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt   = datetime.strptime(end_date,   '%Y-%m-%d')\n",
    "\n",
    "    # 4) Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "    out_dir = os.path.join(os.getcwd(), 'downloads', symbol)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    csv_files = []\n",
    "    current = start_dt\n",
    "    while current <= end_dt:\n",
    "        # Ø±Ø§Ø¨Ø· Ø§Ù„ØªÙ†Ø²ÙŠÙ„ ÙˆØ§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù„ÙŠ\n",
    "        url = build_bi5_url(symbol, data_type, current, timeframe)\n",
    "        bi5_name = f\"{symbol}_{current.strftime('%Y%m%d')}.bi5\"\n",
    "        bi5_path = os.path.join(out_dir, bi5_name)\n",
    "\n",
    "        print(f\"\\nDownloading {current.strftime('%Y-%m-%d')}...\")\n",
    "        try:\n",
    "            download_bi5(url, bi5_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ÙØ´Ù„ ØªÙ†Ø²ÙŠÙ„ {bi5_name}: {e}\")\n",
    "            current += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        # ØªØ­ÙˆÙŠÙ„ BI5 Ø¥Ù„Ù‰ CSV\n",
    "        csv_name = bi5_name.replace('.bi5', '.csv')\n",
    "        csv_path = os.path.join(out_dir, csv_name)\n",
    "        bi5_to_csv(bi5_path, csv_path, timeframe)\n",
    "        csv_files.append(csv_path)\n",
    "\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "    # 5) Ø¯Ù…Ø¬ ÙƒÙ„ CSV ÙÙŠ Ù…Ù„Ù ÙˆØ§Ø­Ø¯\n",
    "    merged_name = f\"{symbol}_{start_date}_{end_date}_merged.csv\"\n",
    "    merged_path = os.path.join(out_dir, merged_name)\n",
    "    merge_csv(csv_files, merged_path)\n",
    "\n",
    "    print(f\"\\nAll files stored in: {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88e58b05-0e42-4b82-b772-aecc43e1cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select main category:\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter category number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected category: Metals\n",
      "\n",
      "Select instrument:\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter instrument number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected instrument: XAUUSD\n",
      "\n",
      "1. Candlestick\n",
      "2. Tick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose data type (number):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select timeframe:  4\n",
      "Enter start date (YYYY-MM-DD):  2020-01-01\n",
      "Enter end date   (YYYY-MM-DD):  2020-01-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading 2020-01-01...\n",
      "âš ï¸ ØªØ®Ø·Ù‰ 2020-01-01: HTTP 403\n",
      "\n",
      "Downloading 2020-01-02...\n",
      "âš ï¸ ØªØ®Ø·Ù‰ 2020-01-02: HTTP 403\n",
      "\n",
      "Downloading 2020-01-03...\n",
      "âš ï¸ ØªØ®Ø·Ù‰ 2020-01-03: HTTP 403\n",
      "\n",
      "Downloading 2020-01-04...\n",
      "âš ï¸ ØªØ®Ø·Ù‰ 2020-01-04: HTTP 403\n",
      "âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª CSV Ù„Ø¯Ù…Ø¬Ù‡Ø§.\n",
      "\n",
      "All files stored in: C:\\Users\\Access\\downloads\\XAUUSD\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import lzma\n",
    "import struct\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "BASE_URL = 'https://www.dukascopy.com/datafeed'\n",
    "CATEGORIES = {\n",
    "    'Metals': ['XAUUSD', 'XAGUSD'],\n",
    "    'Forex' : ['EURUSD', 'GBPUSD', 'USDJPY', 'USDCHF', 'AUDUSD'],\n",
    "    'Indices': ['DAX', 'SP500'],\n",
    "}\n",
    "TIMEFRAMES = ['1m', '5m', '15m', '1h', '4h', '1d']\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def select_option(options, prompt_text):\n",
    "    for idx, opt in enumerate(options, 1):\n",
    "        print(f\"{idx}. {opt}\")\n",
    "    while True:\n",
    "        choice = input(prompt_text).strip()\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(options):\n",
    "            return options[int(choice) - 1]\n",
    "        print(\"Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ§Ù„Ø­ØŒ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\")\n",
    "\n",
    "def build_bi5_url(symbol, data_type, date_dt, timeframe=None):\n",
    "    y, m, d = date_dt.year, f\"{date_dt.month:02}\", f\"{date_dt.day:02}\"\n",
    "    if data_type == 'Tick':\n",
    "        return f\"{BASE_URL}/{symbol}/{y}/{m}/{d}/ticks.bi5\"\n",
    "    else:\n",
    "        return f\"{BASE_URL}/{symbol}/{timeframe}/{y}/{m}/{d}/{timeframe}.bi5\"\n",
    "\n",
    "def download_bi5(url, dest_path):\n",
    "    \"\"\"\n",
    "    ÙŠÙ†Ø²Ù„ Ù…Ù„Ù bi5. Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ (404) Ø£Ùˆ Ø­Ø¬Ù…Ù‡ ØµÙØ±ØŒ ÙŠØ±ÙØ¹ Ø§Ø³ØªØ«Ù†Ø§Ø¡.\n",
    "    \"\"\"\n",
    "    resp = requests.get(url, stream=True)\n",
    "    if resp.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {resp.status_code}\")\n",
    "    # Ø­Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø¡Ø© header Ø§Ù„Ø­Ø¬Ù…:\n",
    "    total = resp.headers.get('content-length')\n",
    "    total = int(total) if total and total.isdigit() else None\n",
    "\n",
    "    with open(dest_path, 'wb') as f, tqdm(\n",
    "        total=total, unit='iB', unit_scale=True,\n",
    "        desc=os.path.basename(dest_path),\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    ) as bar:\n",
    "        for chunk in resp.iter_content(1024):\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "    # ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù„ÙŠØ³ ÙØ§Ø±ØºØ§Ù‹\n",
    "    if os.path.getsize(dest_path) == 0:\n",
    "        os.remove(dest_path)\n",
    "        raise RuntimeError(\"Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº\")\n",
    "\n",
    "def bi5_to_csv(bi5_path, csv_path, timeframe):\n",
    "    tf_ms = {\n",
    "        '1m': 60_000, '5m': 5*60_000, '15m': 15*60_000,\n",
    "        '1h': 3600_000, '4h': 4*3600_000, '1d': 86400_000\n",
    "    }[timeframe]\n",
    "    with lzma.open(bi5_path) as fin, open(csv_path, 'w', newline='') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow(['Gmt time','Open','High','Low','Close','Volume'])\n",
    "        record_size = 8 + 5*4\n",
    "        while True:\n",
    "            chunk = fin.read(record_size)\n",
    "            if len(chunk) < record_size:\n",
    "                break\n",
    "            t_ms, o, h, l, c, v = struct.unpack('>Qffffi', chunk)\n",
    "            t_str = datetime.utcfromtimestamp(t_ms/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            writer.writerow([t_str, o, h, l, c, v])\n",
    "\n",
    "def merge_csv(csv_files, out_path):\n",
    "    if not csv_files:\n",
    "        print(\"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª CSV Ù„Ø¯Ù…Ø¬Ù‡Ø§.\")\n",
    "        return\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        dfs.append(pd.read_csv(f, parse_dates=['Gmt time']))\n",
    "    all_df = pd.concat(dfs, ignore_index=True)\n",
    "    all_df.sort_values('Gmt time', inplace=True)\n",
    "    all_df.to_csv(out_path, index=False)\n",
    "    print(f\"\\nâœ”ï¸ ØªÙ… Ø§Ù„Ø¯Ù…Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {out_path}\")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def main():\n",
    "    print(\"Select main category:\")\n",
    "    category = select_option(list(CATEGORIES.keys()), \"Enter category number: \")\n",
    "    print(f\"Selected category: {category}\\n\")\n",
    "\n",
    "    print(\"Select instrument:\")\n",
    "    symbol = select_option(CATEGORIES[category], \"Enter instrument number: \")\n",
    "    print(f\"Selected instrument: {symbol}\\n\")\n",
    "\n",
    "    data_type = select_option(['Candlestick', 'Tick'], \"Choose data type (number): \")\n",
    "    if data_type == 'Candlestick':\n",
    "        timeframe = select_option(TIMEFRAMES, \"Select timeframe: \")\n",
    "    else:\n",
    "        timeframe = '1m'  # Ø¥Ø·Ø§Ø± ÙˆÙ‡Ù…ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙŠÙƒ Ø¥Ù„Ù‰ CSV\n",
    "\n",
    "    start_dt = datetime.strptime(input(\"Enter start date (YYYY-MM-DD): \").strip(), '%Y-%m-%d')\n",
    "    end_dt   = datetime.strptime(input(\"Enter end date   (YYYY-MM-DD): \").strip(), '%Y-%m-%d')\n",
    "\n",
    "    out_dir = os.path.join(os.getcwd(), 'downloads', symbol)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    csv_files = []\n",
    "    current = start_dt\n",
    "    while current <= end_dt:\n",
    "        print(f\"\\nDownloading {current.strftime('%Y-%m-%d')}...\")\n",
    "        url      = build_bi5_url(symbol, data_type, current, timeframe)\n",
    "        bi5_file = os.path.join(out_dir, f\"{symbol}_{current.strftime('%Y%m%d')}.bi5\")\n",
    "\n",
    "        try:\n",
    "            download_bi5(url, bi5_file)\n",
    "            # ØªØ­ÙˆÙŠÙ„ ÙˆÙÙƒ Ø§Ù„Ø¶ØºØ·\n",
    "            csv_file = bi5_file.replace('.bi5', '.csv')\n",
    "            bi5_to_csv(bi5_file, csv_file, timeframe)\n",
    "            csv_files.append(csv_file)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ØªØ®Ø·Ù‰ {current.strftime('%Y-%m-%d')}: {e}\")\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "    # Ø¯Ù…Ø¬ ÙˆØ¥Ù†Ù‡Ø§Ø¡\n",
    "    merged_name = f\"{symbol}_{start_dt.date()}_{end_dt.date()}_merged.csv\"\n",
    "    merge_csv(csv_files, os.path.join(out_dir, merged_name))\n",
    "    print(f\"\\nAll files stored in: {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e05c2c6b-916b-469a-b8b3-a50b9dafb8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select main category:\n",
      "1. Metals\n",
      "2. Forex\n",
      "3. Indices\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter category number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected category: Metals\n",
      "\n",
      "Select instrument:\n",
      "1. XAUUSD\n",
      "2. XAGUSD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter instrument number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected instrument: XAUUSD\n",
      "\n",
      "1. Candlestick\n",
      "2. Tick\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose data type (number):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1m\n",
      "2. 5m\n",
      "3. 15m\n",
      "4. 1h\n",
      "5. 4h\n",
      "6. 1d\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select timeframe:  3\n",
      "Enter start date (YYYY-MM-DD):  2020-01-01\n",
      "Enter end date   (YYYY-MM-DD):  2020-01-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading 2020-01-01...\n",
      "âš ï¸ ØªØ®Ø·Ù‰ 2020-01-01: HTTP 404\n",
      "\n",
      "Downloading 2020-01-02...\n",
      "âš ï¸ ØªØ®Ø·Ù‰ 2020-01-02: HTTP 404\n",
      "âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª CSV Ù„Ø¯Ù…Ø¬Ù‡Ø§.\n",
      "\n",
      "All files stored in: C:\\Users\\Access\\downloads\\XAUUSD\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import lzma\n",
    "import struct\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "BASE_URL = 'https://datafeed.dukascopy.com/datafeed'\n",
    "CATEGORIES = {\n",
    "    'Metals': ['XAUUSD', 'XAGUSD'],\n",
    "    'Forex' : ['EURUSD', 'GBPUSD', 'USDJPY', 'USDCHF', 'AUDUSD'],\n",
    "    'Indices': ['DAX', 'SP500'],\n",
    "}\n",
    "TIMEFRAMES = ['1m', '5m', '15m', '1h', '4h', '1d']\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ù„Ø³Ø© Ù…Ø¹ Ø§Ù„Ø±Ø¤ÙˆØ³ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/115.0.0.0 Safari/537.36',\n",
    "    'Referer': 'https://www.dukascopy.com/trading-tools/widgets/quotes/historical_data_feed'\n",
    "})\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def select_option(options, prompt_text):\n",
    "    for idx, opt in enumerate(options, 1):\n",
    "        print(f\"{idx}. {opt}\")\n",
    "    while True:\n",
    "        choice = input(prompt_text).strip()\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(options):\n",
    "            return options[int(choice) - 1]\n",
    "        print(\"Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ§Ù„Ø­ØŒ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\")\n",
    "\n",
    "def build_bi5_url(symbol, data_type, date_dt, timeframe=None):\n",
    "    y, m, d = date_dt.year, f\"{date_dt.month:02}\", f\"{date_dt.day:02}\"\n",
    "    if data_type == 'Tick':\n",
    "        return f\"{BASE_URL}/{symbol}/{y}/{m}/{d}/ticks.bi5\"\n",
    "    else:\n",
    "        return f\"{BASE_URL}/{symbol}/{timeframe}/{y}/{m}/{d}/{timeframe}.bi5\"\n",
    "\n",
    "def download_bi5(url, dest_path):\n",
    "    \"\"\"\n",
    "    ÙŠÙ†Ø²Ù„ Ù…Ù„Ù bi5. Ø¥Ø°Ø§ ÙƒØ§Ù† ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ (404) Ø£Ùˆ ÙØ§Ø±ØºÙ‹Ø§ØŒ ÙŠØ±ÙØ¹ Ø§Ø³ØªØ«Ù†Ø§Ø¡.\n",
    "    \"\"\"\n",
    "    resp = session.get(url, stream=True)\n",
    "    if resp.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {resp.status_code}\")\n",
    "    total = resp.headers.get('content-length')\n",
    "    total = int(total) if total and total.isdigit() else None\n",
    "\n",
    "    with open(dest_path, 'wb') as f, tqdm(\n",
    "        total=total, unit='iB', unit_scale=True,\n",
    "        desc=os.path.basename(dest_path),\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "    ) as bar:\n",
    "        for chunk in resp.iter_content(1024):\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "    if os.path.getsize(dest_path) == 0:\n",
    "        os.remove(dest_path)\n",
    "        raise RuntimeError(\"Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº\")\n",
    "\n",
    "def bi5_to_csv(bi5_path, csv_path, timeframe):\n",
    "    tf_ms = {\n",
    "        '1m': 60_000, '5m': 5*60_000, '15m': 15*60_000,\n",
    "        '1h': 3600_000, '4h': 4*3600_000, '1d': 86400_000\n",
    "    }[timeframe]\n",
    "    with lzma.open(bi5_path) as fin, open(csv_path, 'w', newline='') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow(['Gmt time','Open','High','Low','Close','Volume'])\n",
    "        record_size = 8 + 5*4\n",
    "        while True:\n",
    "            chunk = fin.read(record_size)\n",
    "            if len(chunk) < record_size:\n",
    "                break\n",
    "            t_ms, o, h, l, c, v = struct.unpack('>Qffffi', chunk)\n",
    "            t_str = datetime.utcfromtimestamp(t_ms/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            writer.writerow([t_str, o, h, l, c, v])\n",
    "\n",
    "def merge_csv(csv_files, out_path):\n",
    "    if not csv_files:\n",
    "        print(\"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª CSV Ù„Ø¯Ù…Ø¬Ù‡Ø§.\")\n",
    "        return\n",
    "    dfs = [pd.read_csv(f, parse_dates=['Gmt time']) for f in csv_files]\n",
    "    all_df = pd.concat(dfs, ignore_index=True)\n",
    "    all_df.sort_values('Gmt time', inplace=True)\n",
    "    all_df.to_csv(out_path, index=False)\n",
    "    print(f\"\\nâœ”ï¸ ØªÙ… Ø§Ù„Ø¯Ù…Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {out_path}\")\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def main():\n",
    "    print(\"Select main category:\")\n",
    "    category = select_option(list(CATEGORIES.keys()), \"Enter category number: \")\n",
    "    print(f\"Selected category: {category}\\n\")\n",
    "\n",
    "    print(\"Select instrument:\")\n",
    "    symbol = select_option(CATEGORIES[category], \"Enter instrument number: \")\n",
    "    print(f\"Selected instrument: {symbol}\\n\")\n",
    "\n",
    "    data_type = select_option(['Candlestick', 'Tick'], \"Choose data type (number): \")\n",
    "    timeframe = (select_option(TIMEFRAMES, \"Select timeframe: \")\n",
    "                 if data_type=='Candlestick' else '1m')\n",
    "\n",
    "    start_dt = datetime.strptime(input(\"Enter start date (YYYY-MM-DD): \").strip(), '%Y-%m-%d')\n",
    "    end_dt   = datetime.strptime(input(\"Enter end date   (YYYY-MM-DD): \").strip(), '%Y-%m-%d')\n",
    "\n",
    "    out_dir = os.path.join(os.getcwd(), 'downloads', symbol)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    csv_files = []\n",
    "    current = start_dt\n",
    "    while current <= end_dt:\n",
    "        print(f\"\\nDownloading {current.strftime('%Y-%m-%d')}...\")\n",
    "        url      = build_bi5_url(symbol, data_type, current, timeframe)\n",
    "        bi5_file = os.path.join(out_dir, f\"{symbol}_{current.strftime('%Y%m%d')}.bi5\")\n",
    "\n",
    "        try:\n",
    "            download_bi5(url, bi5_file)\n",
    "            csv_file = bi5_file.replace('.bi5', '.csv')\n",
    "            bi5_to_csv(bi5_file, csv_file, timeframe)\n",
    "            csv_files.append(csv_file)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ØªØ®Ø·Ù‰ {current.strftime('%Y-%m-%d')}: {e}\")\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "    merged_name = f\"{symbol}_{start_dt.date()}_{end_dt.date()}_merged.csv\"\n",
    "    merge_csv(csv_files, os.path.join(out_dir, merged_name))\n",
    "    print(f\"\\nAll files stored in: {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6cfe6-0118-4300-9469-a6c91d5f9461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
