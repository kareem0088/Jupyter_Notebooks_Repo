{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5f17bc-f5fe-4dc9-bff6-5e93d0fc36f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting USD Economic Calendar scraping from 2020 to 2025\n",
      "Trying API methods...\n",
      "Trying alternative sources...\n",
      "No real data found, creating sample data...\n",
      "Creating sample USD economic calendar data...\n",
      "Successfully processed 720 USD economic events\n",
      "\n",
      "=== ملخص بيانات الأجندة الاقتصادية للدولار ===\n",
      "إجمالي الأحداث: 720\n",
      "النطاق الزمني: 2020-01-01 إلى 2025-12-20\n",
      "مستويات الأهمية: {'Medium': 504, 'High': 216}\n",
      "\n",
      "=== أهم 10 أحداث ===\n",
      "      date                             event_name importance_level actual\n",
      "2020-01-01                      Non-Farm Payrolls             High   0.2%\n",
      "2020-01-02 Federal Reserve Interest Rate Decision             High   0.3%\n",
      "2020-01-03             Consumer Price Index (CPI)             High   0.4%\n",
      "2020-01-04             Producer Price Index (PPI)           Medium   0.5%\n",
      "2020-01-05                        GDP Growth Rate           Medium   0.6%\n",
      "2020-01-06                      Unemployment Rate           Medium   0.7%\n",
      "2020-01-07                           Retail Sales           Medium   0.8%\n",
      "2020-01-08                  Industrial Production           Medium   0.9%\n",
      "2020-01-09                    Consumer Confidence           Medium   1.0%\n",
      "2020-01-10                  ISM Manufacturing PMI           Medium   1.1%\n",
      "Data saved to usd_economic_calendar_2020_2025.csv\n",
      "Data also saved to usd_economic_calendar_2020_2025.json\n",
      "Data also saved to usd_economic_calendar_2020_2025.xlsx\n",
      "Data saved to usd_high_impact_events_2020_2025.csv\n",
      "Data also saved to usd_high_impact_events_2020_2025.json\n",
      "Data also saved to usd_high_impact_events_2020_2025.xlsx\n",
      "تم حفظ الأحداث عالية التأثير: 216 حدث\n",
      "\n",
      "=== إحصائيات إضافية ===\n",
      "الأحداث حسب المصدر:\n",
      "  Sample Data: 720 حدث\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class USDEconomicCalendarScraper:\n",
    "    def __init__(self, use_selenium=True):\n",
    "        self.use_selenium = use_selenium\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'ar,en-US;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Cache-Control': 'max-age=0'\n",
    "        }\n",
    "        \n",
    "        if self.use_selenium:\n",
    "            self.setup_selenium()\n",
    "        else:\n",
    "            self.session = requests.Session()\n",
    "            self.session.headers.update(self.headers)\n",
    "    \n",
    "    def setup_selenium(self):\n",
    "        \"\"\"Setup Selenium WebDriver\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')  # تشغيل في الخلفية\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        chrome_options.add_argument(f'--user-agent={self.headers[\"User-Agent\"]}')\n",
    "        \n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up Selenium: {e}\")\n",
    "            print(\"Falling back to requests method...\")\n",
    "            self.use_selenium = False\n",
    "            self.session = requests.Session()\n",
    "            self.session.headers.update(self.headers)\n",
    "    \n",
    "    def scrape_investing_api(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        استخدام API من Investing.com للحصول على البيانات\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        try:\n",
    "            # استخدام API endpoint للتقويم الاقتصادي\n",
    "            api_url = \"https://api.investing.com/api/financialdata/assets/economicCalendar\"\n",
    "            \n",
    "            # معاملات API\n",
    "            params = {\n",
    "                'from': start_date.strftime('%Y-%m-%d'),\n",
    "                'to': end_date.strftime('%Y-%m-%d'),\n",
    "                'countries': 'united-states',\n",
    "                'importance': '2,3',  # متوسط وعالي\n",
    "                'currencies': 'USD'\n",
    "            }\n",
    "            \n",
    "            headers = self.headers.copy()\n",
    "            headers.update({\n",
    "                'X-Requested-With': 'XMLHttpRequest',\n",
    "                'Referer': 'https://www.investing.com/economic-calendar/',\n",
    "                'Accept': 'application/json, text/javascript, */*; q=0.01'\n",
    "            })\n",
    "            \n",
    "            response = requests.get(api_url, params=params, headers=headers, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                json_data = response.json()\n",
    "                if 'data' in json_data:\n",
    "                    for event in json_data['data']:\n",
    "                        event_data = {\n",
    "                            'date': event.get('date', ''),\n",
    "                            'time': event.get('time', ''),\n",
    "                            'event_name': event.get('event', ''),\n",
    "                            'country': 'USD',\n",
    "                            'importance': int(event.get('importance', 0)),\n",
    "                            'previous': event.get('previous', ''),\n",
    "                            'forecast': event.get('forecast', ''),\n",
    "                            'actual': event.get('actual', ''),\n",
    "                            'source': 'Investing.com API'\n",
    "                        }\n",
    "                        data.append(event_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with Investing API: {e}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def scrape_alternative_sources(self):\n",
    "        \"\"\"\n",
    "        استخدام مصادر بديلة للبيانات الاقتصادية\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        # مصدر 1: Economic Calendar من MarketWatch\n",
    "        try:\n",
    "            marketwatch_data = self.scrape_marketwatch()\n",
    "            data.extend(marketwatch_data)\n",
    "        except Exception as e:\n",
    "            print(f\"MarketWatch scraping failed: {e}\")\n",
    "        \n",
    "        # مصدر 2: Yahoo Finance Economic Calendar\n",
    "        try:\n",
    "            yahoo_data = self.scrape_yahoo_finance()\n",
    "            data.extend(yahoo_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Yahoo Finance scraping failed: {e}\")\n",
    "        \n",
    "        # مصدر 3: TradingView Economic Calendar\n",
    "        try:\n",
    "            tradingview_data = self.scrape_tradingview()\n",
    "            data.extend(tradingview_data)\n",
    "        except Exception as e:\n",
    "            print(f\"TradingView scraping failed: {e}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def scrape_marketwatch(self):\n",
    "        \"\"\"\n",
    "        Scrape من MarketWatch Economic Calendar\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            url = \"https://www.marketwatch.com/economy-politics/calendar\"\n",
    "            \n",
    "            if self.use_selenium:\n",
    "                self.driver.get(url)\n",
    "                time.sleep(3)\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            else:\n",
    "                response = self.session.get(url, timeout=30)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # البحث عن جدول الأحداث الاقتصادية\n",
    "            calendar_table = soup.find('table', class_='table--economic-calendar')\n",
    "            if calendar_table:\n",
    "                rows = calendar_table.find_all('tr')[1:]  # تجاهل الهيدر\n",
    "                \n",
    "                for row in rows:\n",
    "                    cells = row.find_all('td')\n",
    "                    if len(cells) >= 6:\n",
    "                        event_data = {\n",
    "                            'date': cells[0].get_text(strip=True),\n",
    "                            'time': cells[1].get_text(strip=True),\n",
    "                            'event_name': cells[2].get_text(strip=True),\n",
    "                            'country': 'USD',\n",
    "                            'importance': self.parse_importance(cells[3]),\n",
    "                            'previous': cells[4].get_text(strip=True),\n",
    "                            'forecast': cells[5].get_text(strip=True),\n",
    "                            'actual': cells[6].get_text(strip=True) if len(cells) > 6 else '',\n",
    "                            'source': 'MarketWatch'\n",
    "                        }\n",
    "                        data.append(event_data)\n",
    "        except Exception as e:\n",
    "            print(f\"MarketWatch error: {e}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def scrape_yahoo_finance(self):\n",
    "        \"\"\"\n",
    "        Scrape من Yahoo Finance Economic Calendar\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            url = \"https://finance.yahoo.com/calendar/economic\"\n",
    "            \n",
    "            if self.use_selenium:\n",
    "                self.driver.get(url)\n",
    "                time.sleep(3)\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            else:\n",
    "                response = self.session.get(url, timeout=30)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # البحث عن البيانات\n",
    "            events = soup.find_all('tr', {'data-test': 'calendar-row'})\n",
    "            \n",
    "            for event in events:\n",
    "                try:\n",
    "                    cells = event.find_all('td')\n",
    "                    if len(cells) >= 4:\n",
    "                        event_name = cells[0].get_text(strip=True)\n",
    "                        if 'USD' in event_name or 'US' in event_name:\n",
    "                            event_data = {\n",
    "                                'date': cells[1].get_text(strip=True),\n",
    "                                'time': cells[2].get_text(strip=True),\n",
    "                                'event_name': event_name,\n",
    "                                'country': 'USD',\n",
    "                                'importance': 2,  # افتراضي متوسط\n",
    "                                'previous': cells[3].get_text(strip=True) if len(cells) > 3 else '',\n",
    "                                'forecast': cells[4].get_text(strip=True) if len(cells) > 4 else '',\n",
    "                                'actual': cells[5].get_text(strip=True) if len(cells) > 5 else '',\n",
    "                                'source': 'Yahoo Finance'\n",
    "                            }\n",
    "                            data.append(event_data)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Yahoo Finance error: {e}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def scrape_tradingview(self):\n",
    "        \"\"\"\n",
    "        Scrape من TradingView Economic Calendar\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            url = \"https://www.tradingview.com/economic-calendar/\"\n",
    "            \n",
    "            if self.use_selenium:\n",
    "                self.driver.get(url)\n",
    "                time.sleep(5)  # وقت أطول للتحميل\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            else:\n",
    "                response = self.session.get(url, timeout=30)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # البحث عن البيانات في TradingView\n",
    "            events = soup.find_all('tr', class_='row-RdUXZpkv')\n",
    "            \n",
    "            for event in events:\n",
    "                try:\n",
    "                    # استخراج بيانات الحدث\n",
    "                    country_cell = event.find('span', class_='currency-BbubK_kl')\n",
    "                    if country_cell and 'USD' in country_cell.get_text():\n",
    "                        \n",
    "                        time_cell = event.find('time')\n",
    "                        event_cell = event.find('a', class_='title-BbubK_kl')\n",
    "                        impact_cell = event.find('span', class_='impact-BbubK_kl')\n",
    "                        \n",
    "                        event_data = {\n",
    "                            'date': time_cell.get('datetime', '') if time_cell else '',\n",
    "                            'time': time_cell.get_text(strip=True) if time_cell else '',\n",
    "                            'event_name': event_cell.get_text(strip=True) if event_cell else '',\n",
    "                            'country': 'USD',\n",
    "                            'importance': self.parse_tradingview_impact(impact_cell),\n",
    "                            'previous': '',\n",
    "                            'forecast': '',\n",
    "                            'actual': '',\n",
    "                            'source': 'TradingView'\n",
    "                        }\n",
    "                        \n",
    "                        # البحث عن القيم\n",
    "                        value_cells = event.find_all('span', class_='value-BbubK_kl')\n",
    "                        if len(value_cells) >= 3:\n",
    "                            event_data['actual'] = value_cells[0].get_text(strip=True)\n",
    "                            event_data['forecast'] = value_cells[1].get_text(strip=True)\n",
    "                            event_data['previous'] = value_cells[2].get_text(strip=True)\n",
    "                        \n",
    "                        data.append(event_data)\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"TradingView error: {e}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def parse_importance(self, cell):\n",
    "        \"\"\"تحليل مستوى الأهمية من الخلية\"\"\"\n",
    "        text = cell.get_text(strip=True).lower()\n",
    "        if 'high' in text or 'عالي' in text:\n",
    "            return 3\n",
    "        elif 'medium' in text or 'متوسط' in text:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def parse_tradingview_impact(self, cell):\n",
    "        \"\"\"تحليل مستوى التأثير في TradingView\"\"\"\n",
    "        if not cell:\n",
    "            return 1\n",
    "        \n",
    "        class_names = cell.get('class', [])\n",
    "        if 'high' in ' '.join(class_names).lower():\n",
    "            return 3\n",
    "        elif 'medium' in ' '.join(class_names).lower():\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"\n",
    "        إنشاء بيانات عينة للأجندة الاقتصادية للدولار\n",
    "        \"\"\"\n",
    "        print(\"Creating sample USD economic calendar data...\")\n",
    "        \n",
    "        # أحداث اقتصادية مهمة للدولار\n",
    "        important_events = [\n",
    "            'Non-Farm Payrolls',\n",
    "            'Federal Reserve Interest Rate Decision',\n",
    "            'Consumer Price Index (CPI)',\n",
    "            'Producer Price Index (PPI)',\n",
    "            'GDP Growth Rate',\n",
    "            'Unemployment Rate',\n",
    "            'Retail Sales',\n",
    "            'Industrial Production',\n",
    "            'Consumer Confidence',\n",
    "            'ISM Manufacturing PMI',\n",
    "            'ISM Services PMI',\n",
    "            'FOMC Meeting Minutes',\n",
    "            'Core PCE Price Index',\n",
    "            'Initial Jobless Claims',\n",
    "            'Durable Goods Orders',\n",
    "            'Housing Starts',\n",
    "            'Existing Home Sales',\n",
    "            'New Home Sales',\n",
    "            'Trade Balance',\n",
    "            'Factory Orders'\n",
    "        ]\n",
    "        \n",
    "        data = []\n",
    "        start_date = datetime(2020, 1, 1)\n",
    "        end_date = datetime(2025, 12, 31)\n",
    "        \n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            # إضافة أحداث شهرية\n",
    "            for event in important_events[:10]:  # أهم 10 أحداث\n",
    "                if current_date.day <= 28:  # تجنب مشاكل نهاية الشهر\n",
    "                    event_date = current_date.replace(day=min(28, current_date.day + len(data) % 28))\n",
    "                    \n",
    "                    # تحديد مستوى الأهمية\n",
    "                    if event in ['Non-Farm Payrolls', 'Federal Reserve Interest Rate Decision', 'Consumer Price Index (CPI)']:\n",
    "                        importance = 3  # عالي\n",
    "                    else:\n",
    "                        importance = 2  # متوسط\n",
    "                    \n",
    "                    event_data = {\n",
    "                        'date': event_date.strftime('%Y-%m-%d'),\n",
    "                        'time': f\"{9 + (len(data) % 8)}:30\",\n",
    "                        'event_name': event,\n",
    "                        'country': 'USD',\n",
    "                        'importance': importance,\n",
    "                        'previous': f'{(len(data) % 100) / 10:.1f}%',\n",
    "                        'forecast': f'{((len(data) + 1) % 100) / 10:.1f}%',\n",
    "                        'actual': f'{((len(data) + 2) % 100) / 10:.1f}%',\n",
    "                        'source': 'Sample Data'\n",
    "                    }\n",
    "                    data.append(event_data)\n",
    "            \n",
    "            # الانتقال للشهر التالي\n",
    "            if current_date.month == 12:\n",
    "                current_date = current_date.replace(year=current_date.year + 1, month=1)\n",
    "            else:\n",
    "                current_date = current_date.replace(month=current_date.month + 1)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def scrape_economic_calendar(self, start_year=2020, end_year=2025):\n",
    "        \"\"\"\n",
    "        الدالة الرئيسية لجمع بيانات الأجندة الاقتصادية\n",
    "        \"\"\"\n",
    "        print(f\"Starting USD Economic Calendar scraping from {start_year} to {end_year}\")\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        # محاولة الطرق المختلفة\n",
    "        print(\"Trying API methods...\")\n",
    "        start_date = datetime(start_year, 1, 1)\n",
    "        end_date = datetime(end_year, 12, 31)\n",
    "        \n",
    "        # محاولة API\n",
    "        api_data = self.scrape_investing_api(start_date, end_date)\n",
    "        if api_data:\n",
    "            all_data.extend(api_data)\n",
    "            print(f\"Got {len(api_data)} events from API\")\n",
    "        \n",
    "        # محاولة المصادر البديلة\n",
    "        print(\"Trying alternative sources...\")\n",
    "        alt_data = self.scrape_alternative_sources()\n",
    "        if alt_data:\n",
    "            all_data.extend(alt_data)\n",
    "            print(f\"Got {len(alt_data)} events from alternative sources\")\n",
    "        \n",
    "        # إذا لم نحصل على بيانات، ننشئ بيانات عينة\n",
    "        if not all_data:\n",
    "            print(\"No real data found, creating sample data...\")\n",
    "            all_data = self.create_sample_data()\n",
    "        \n",
    "        # تحويل إلى DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # تنظيف ومعالجة البيانات\n",
    "            df = self.clean_data(df)\n",
    "            \n",
    "            # فلترة للأحداث متوسطة وعالية الأهمية فقط\n",
    "            df = df[df['importance'] >= 2]\n",
    "            \n",
    "            # ترتيب حسب التاريخ\n",
    "            df = df.sort_values('date')\n",
    "            \n",
    "            print(f\"Successfully processed {len(df)} USD economic events\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No data found\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def clean_data(self, df):\n",
    "        \"\"\"\n",
    "        تنظيف ومعالجة البيانات\n",
    "        \"\"\"\n",
    "        # إزالة التكرارات\n",
    "        df = df.drop_duplicates(subset=['date', 'event_name'], keep='first')\n",
    "        \n",
    "        # تنظيف القيم\n",
    "        df['previous'] = df['previous'].replace('', None)\n",
    "        df['forecast'] = df['forecast'].replace('', None)\n",
    "        df['actual'] = df['actual'].replace('', None)\n",
    "        \n",
    "        # تحويل مستوى الأهمية إلى نص\n",
    "        df['importance_level'] = df['importance'].map({\n",
    "            1: 'Low',\n",
    "            2: 'Medium', \n",
    "            3: 'High'\n",
    "        })\n",
    "        \n",
    "        # إضافة عمود التاريخ والوقت مجمعين\n",
    "        df['datetime'] = df.apply(lambda row: f\"{row['date']} {row.get('time', '09:30')}\", axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_data(self, df, filename='usd_economic_calendar.csv'):\n",
    "        \"\"\"\n",
    "        حفظ البيانات في ملفات مختلفة\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # حفظ كـ CSV\n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"Data saved to {filename}\")\n",
    "            \n",
    "            # حفظ كـ JSON\n",
    "            json_filename = filename.replace('.csv', '.json')\n",
    "            df.to_json(json_filename, orient='records', date_format='iso', indent=2)\n",
    "            print(f\"Data also saved to {json_filename}\")\n",
    "            \n",
    "            # حفظ كـ Excel\n",
    "            excel_filename = filename.replace('.csv', '.xlsx')\n",
    "            with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "                df.to_excel(writer, sheet_name='Economic Calendar', index=False)\n",
    "                \n",
    "                # ورقة منفصلة للأحداث عالية الأهمية\n",
    "                high_impact = df[df['importance'] == 3]\n",
    "                if not high_impact.empty:\n",
    "                    high_impact.to_excel(writer, sheet_name='High Impact Events', index=False)\n",
    "            \n",
    "            print(f\"Data also saved to {excel_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"تنظيف الموارد\"\"\"\n",
    "        if hasattr(self, 'driver') and self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# مثال على الاستخدام\n",
    "if __name__ == \"__main__\":\n",
    "    # إنشاء مثيل من الـ scraper\n",
    "    scraper = USDEconomicCalendarScraper(use_selenium=False)  # ابدأ بدون Selenium\n",
    "    \n",
    "    try:\n",
    "        # جمع البيانات من 2020 إلى 2025\n",
    "        df = scraper.scrape_economic_calendar(start_year=2020, end_year=2025)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # عرض إحصائيات أساسية\n",
    "            print(\"\\n=== ملخص بيانات الأجندة الاقتصادية للدولار ===\")\n",
    "            print(f\"إجمالي الأحداث: {len(df)}\")\n",
    "            print(f\"النطاق الزمني: {df['date'].min()} إلى {df['date'].max()}\")\n",
    "            print(f\"مستويات الأهمية: {df['importance_level'].value_counts().to_dict()}\")\n",
    "            \n",
    "            # عرض أهم الأحداث\n",
    "            print(\"\\n=== أهم 10 أحداث ===\")\n",
    "            display_cols = ['date', 'event_name', 'importance_level', 'actual']\n",
    "            print(df[display_cols].head(10).to_string(index=False))\n",
    "            \n",
    "            # حفظ البيانات\n",
    "            scraper.save_data(df, 'usd_economic_calendar_2020_2025.csv')\n",
    "            \n",
    "            # حفظ بيانات مفلترة (الأحداث عالية التأثير فقط)\n",
    "            high_impact_df = df[df['importance'] == 3]\n",
    "            if not high_impact_df.empty:\n",
    "                scraper.save_data(high_impact_df, 'usd_high_impact_events_2020_2025.csv')\n",
    "                print(f\"تم حفظ الأحداث عالية التأثير: {len(high_impact_df)} حدث\")\n",
    "            \n",
    "            # إحصائيات إضافية\n",
    "            print(f\"\\n=== إحصائيات إضافية ===\")\n",
    "            print(f\"الأحداث حسب المصدر:\")\n",
    "            source_counts = df['source'].value_counts()\n",
    "            for source, count in source_counts.items():\n",
    "                print(f\"  {source}: {count} حدث\")\n",
    "                \n",
    "        else:\n",
    "            print(\"لم يتم العثور على بيانات. يرجى التحقق من الاتصال بالإنترنت والمحاولة مرة أخرى.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"خطأ في تشغيل البرنامج: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # تنظيف الموارد\n",
    "        del scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39408bac-213e-42c9-952d-87eb7eae7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "يب"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
